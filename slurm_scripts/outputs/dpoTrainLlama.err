wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbarrutia006 (jbarrutia006-upv-ehu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /sorgin1/users/jbarrutia006/viper/wandb/run-20250524_114628-r4kd6s2m
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run First Llama instruct. Dataset: all, bsize 32, lr 1e-4, 
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jbarrutia006-upv-ehu/viperDPO
wandb: üöÄ View run at https://wandb.ai/jbarrutia006-upv-ehu/viperDPO/runs/r4kd6s2m
2025-05-24 11:46:29,913 - INFO - Loading model and tokenizer...
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:00<00:02,  1.42it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [00:01<00:01,  1.24it/s]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [00:02<00:00,  1.20it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.42it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:02<00:00,  1.36it/s]
Unsloth 2025.3.14 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
2025-05-24 11:46:44,908 - INFO - Loading dataset from /sorgin1/users/jbarrutia006/viper/syntData/PrefDatasets/dpo_dataset_all_train.arrow as train and /sorgin1/users/jbarrutia006/viper/syntData/PrefDatasets/dpo_dataset_single_dev.arrow as dev
Map:   0%|          | 0/52487 [00:00<?, ? examples/s]Map:   6%|‚ñå         | 3000/52487 [00:00<00:01, 28790.16 examples/s]Map:  15%|‚ñà‚ñå        | 8000/52487 [00:00<00:01, 36115.07 examples/s]Map:  27%|‚ñà‚ñà‚ñã       | 14000/52487 [00:00<00:00, 38501.62 examples/s]Map:  38%|‚ñà‚ñà‚ñà‚ñä      | 20000/52487 [00:00<00:00, 39396.41 examples/s]Map:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 26000/52487 [00:00<00:00, 39792.77 examples/s]Map:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 30000/52487 [00:00<00:00, 39628.00 examples/s]Map:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 34000/52487 [00:00<00:00, 39691.05 examples/s]Map:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 39000/52487 [00:00<00:00, 39870.88 examples/s]Map:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 44000/52487 [00:01<00:00, 39730.86 examples/s]Map:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 49000/52487 [00:01<00:00, 39769.04 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52487/52487 [00:10<00:00, 4849.86 examples/s] 
Map:   0%|          | 0/1000 [00:00<?, ? examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 5670.44 examples/s]Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 5341.18 examples/s]
2025-05-24 11:46:55,980 - INFO - Train dataset example: {'prompt': [{'content': 'You are an AI that uses a special ImagePatch class to answer questions about images.\nHere is the class definition:\n\nfrom PIL import Image\nfrom vision_functions import find_in_image, simple_qa, verify_property, best_text_match\n\ndef bool_to_yesno(bool_answer: bool)->str:\n    return "yes" if bool_answer else "no"\n\nclass ImagePatch:\n    """A Python class containing a crop of an image centered around a particular object, as well as relevant information.\n    Attributes\n    ----------\n    cropped_image : array_like\n        An array-like of the cropped image taken from the original image.\n    left : int\n        An int describing the position of the left border of the crop\'s bounding box in the original image.\n    lower : int\n        An int describing the position of the bottom border of the crop\'s bounding box in the original image.\n    right : int\n        An int describing the position of the right border of the crop\'s bounding box in the original image.\n    upper : int\n        An int describing the position of the top border of the crop\'s bounding box in the original image.\n\n    Methods\n    -------\n    find(object_name: str)->List[ImagePatch]\n        Returns a list of new ImagePatch objects containing crops of the image centered around any objects found in the image matching the object_name.\n    simple_query(question: str=None)->str\n        Returns the answer to a basic question asked about the image. If no question is provided, returns the answer to "What is this?".\n    exists(object_name: str)->bool\n        Returns True if the object specified by object_name is found in the image, and False otherwise.\n    verify_property(property: str)->bool\n        Returns True if the property is met, and False otherwise.\n    best_text_match(string1: str, string2: str)->str\n        Returns the string that best matches the image.\n    crop(left: int, lower: int, right: int, upper: int)->ImagePatch\n        Returns a new ImagePatch object containing a crop of the image at the given coordinates.\n        """\n\n    def __init__(self, image, left: int=None, lower: int=None, right: int=None, upper: int=None):\n        """Initializes an ImagePatch object by cropping the image at the given coordinates and stores the coordinates as attributes.\n        If no coordinates are provided, the image is left unmodified, and the coordinates are set to the dimensions of the image.\n        Parameters\n        -------\n        image : array_like\n            An array-like of the original image.\n        left : int\n            An int describing the position of the left border of the crop\'s bounding box in the original image.\n        lower : int\n            An int describing the position of the bottom border of the crop\'s bounding box in the original image.\n        right : int\n            An int describing the position of the right border of the crop\'s bounding box in the original image.\n        upper : int\n            An int describing the position of the top border of the crop\'s bounding box in the original image.\n\n        """\n        if left is None and right is None and upper is None and lower is None:\n            self.cropped_image = image\n            self.left = 0\n            self.lower = 0\n            self.right = image.shape[2]  # width\n            self.upper = image.shape[1]  # height\n        else:\n            self.cropped_image = image[:, lower:upper, left:right]\n            self.left = left\n            self.upper = upper\n            self.right = right\n            self.lower = lower\n\n        self.width = self.cropped_image.shape[2]\n        self.height = self.cropped_image.shape[1]\n\n        self.horizontal_center = (self.left + self.right) / 2\n        self.vertical_center = (self.lower + self.upper) / 2\n\n    def find(self, object_name: str)->List["ImagePatch"]:\n        """Returns a new ImagePatch object containing the crop of the image centered around the object specified by object_name.\n        Parameters\n        -------\n        object_name : str\n            A string describing the name of the object to be found in the image.\n        """\n        return find_in_image(self.cropped_image, object_name)\n\n    def simple_query(self, question: str=None)->str:\n        """Returns the answer to a basic question asked about the image. If no question is provided, returns the answer to "What is this?".\n        Parameters\n        -------\n        question : str\n            A string describing the question to be asked.\n\n        Examples\n        -------\n\n        >>> # Which kind of animal is not eating?\n        >>> def execute_command(image)->str:\n        >>>     image_patch = ImagePatch(image)\n        >>>     animal_patches = image_patch.find("animal")\n        >>>     for animal_patch in animal_patches:\n        >>>         if not animal_patch.verify_property("animal", "eating"):\n        >>>             return animal_patch.simple_query("What kind of animal is eating?") # crop would include eating so keep it in the query\n        >>>     # If no animal is not eating, query the image directly\n        >>>     return image_patch.simple_query("Which kind of animal is not eating?")\n\n        >>> # What is in front of the horse?\n        >>> # contains a relation (around, next to, on, near, on top of, in front of, behind, etc), so ask directly\n        >>> return image_patch.simple_query("What is in front of the horse?")\n        >>>\n        """\n        return simple_qa(self.cropped_image, question)\n\n    def exists(self, object_name: str)->bool:\n        """Returns True if the object specified by object_name is found in the image, and False otherwise.\n        Parameters\n        -------\n        object_name : str\n            A string describing the name of the object to be found in the image.\n\n        Examples\n        -------\n        >>> # Are there both cakes and gummy bears in the photo?\n        >>> def execute_command(image)->str:\n        >>>     image_patch = ImagePatch(image)\n        >>>     is_cake = image_patch.exists("cake")\n        >>>     is_gummy_bear = image_patch.exists("gummy bear")\n        >>>     return bool_to_yesno(is_cake and is_gummy_bear)\n        """\n        return len(self.find(object_name)) > 0\n\n    def verify_property(self, object_name: str, property: str)->bool:\n        """Returns True if the object possesses the property, and False otherwise.\n        Differs from \'exists\' in that it presupposes the existence of the object specified by object_name, instead checking whether the object possesses the property.\n        Parameters\n        -------\n        object_name : str\n            A string describing the name of the object to be found in the image.\n        property : str\n            A string describing the property to be checked.\n\n        Examples\n        -------\n        >>> # Do the letters have blue color?\n        >>> def execute_command(image)->str:\n        >>>     image_patch = ImagePatch(image)\n        >>>     letters_patches = image_patch.find("letters")\n        >>>     # Question assumes only one letter patch\n        >>>     if len(letters_patches) == 0:\n        >>>         # If no letters are found, query the image directly\n        >>>         return image_patch.simple_query("Do the letters have blue color?")\n        >>>     return bool_to_yesno(letters_patches[0].verify_property("letters", "blue"))\n        """\n        return verify_property(self.cropped_image, object_name, property)\n\n    def best_text_match(self, option_list: List[str]) -> str:\n        """Returns the string that best matches the image.\n        Parameters\n        -------\n        option_list : str\n            A list with the names of the different options\n        prefix : str\n            A string with the prefixes to append to the options\n\n        Examples\n        -------\n        >>> # Is the cap gold or white?\n        >>> def execute_command(image)->str:\n        >>>     image_patch = ImagePatch(image)\n        >>>     cap_patches = image_patch.find("cap")\n        >>>     # Question assumes one cap patch\n        >>>     if len(cap_patches) == 0:\n        >>>         # If no cap is found, query the image directly\n        >>>         return image_patch.simple_query("Is the cap gold or white?")\n        >>>     return cap_patches[0].best_text_match(["gold", "white"])\n        """\n        return best_text_match(self.cropped_image, option_list)\n\n    def crop(self, left: int, lower: int, right: int, upper: int)->"ImagePatch":\n        """Returns a new ImagePatch cropped from the current ImagePatch.\n        Parameters\n        -------\n        left : int\n            The leftmost pixel of the cropped image.\n        lower : int\n            The lowest pixel of the cropped image.\n        right : int\n            The rightmost pixel of the cropped image.\n        upper : int\n            The uppermost pixel of the cropped image.\n        -------\n        """\n        return ImagePatch(self.cropped_image, left, lower, right, upper)\n\n\n\nPlease use this class to answer queries about images.\nWhen writing the final solution, you typically define a function:\n\ndef execute_command(image)->str:\n    # put your logic here\nYour job is to produce the correct code in that function so that it answers the question or does the operation asked by the user.\n', 'role': 'system'}, {'content': '# Is there a backpack to the right of the man?\ndef execute_command(image)->str:', 'role': 'user'}, {'content': '    image_patch = ImagePatch(image)\n    man_patches = image_patch.find("man")\n    # Question assumes one man patch\n    if len(man_patches) == 0:\n        # If no man is found, query the image directly\n        return image_patch.simple_query("Is there a backpack to the right of the man?")\n    man_patch = man_patches[0]\n    backpack_patches = image_patch.find("backpack")\n    # Question assumes one backpack patch\n    if len(backpack_patches) == 0:\n        return "no"\n    for backpack_patch in backpack_patches:\n        if backpack_patch.horizontal_center > man_patch.horizontal_center:\n            return "yes"\n    return "no"', 'role': 'assistant'}, {'content': '# In which part is the bread, the bottom or the top?\ndef execute_command(image)->str:', 'role': 'user'}, {'content': '    image_patch = ImagePatch(image)\n    bread_patches = image_patch.find("bread")\n    # Question assumes only one bread patch\n    if len(bread_patches) == 0:\n        # If no bread is found, query the image directly\n        return image_patch.simple_query("In which part is the bread, the bottom or the top?")\n    if bread_patches[0].vertical_center < image_patch.vertical_center:\n        return "bottom"\n    else:\n        return "top"', 'role': 'assistant'}, {'content': '# What type of weather do you see in the photograph?\ndef execute_command(image)->str:', 'role': 'user'}, {'content': '    image_patch = ImagePatch(image)\n    return image_patch.simple_query("What type of weather do you see in the photograph?")', 'role': 'assistant'}, {'content': '# Who is the man staring at?\ndef execute_command(image)->str:', 'role': 'user'}, {'content': '    # asks for the predicate of a relational verb (staring at), so ask directly\n    image_patch = ImagePatch(image)\n    return image_patch.simple_query("Who is the man staring at?")', 'role': 'assistant'}, {'content': '# What toy is wearing a shirt?\ndef execute_command(image)->str:', 'role': 'user'}, {'content': '    # not a relational verb so go step by step\n    image_patch = ImagePatch(image)\n    toy_patches = image_patch.find("toy")\n    # Question assumes only one toy patch\n    if len(toy_patches) == 0:\n        # If no toy is found, query the image directly\n        return image_patch.simple_query("What toy is wearing a shirt?")\n    for toy_patch in toy_patches:\n        is_wearing_shirt = (toy_patch.simple_query("Is the toy wearing a shirt?") == "yes")\n        if is_wearing_shirt:\n            return toy_patch.simple_query("What toy is wearing a shirt?") # crop would include the shirt so keep it in the query\n    # If no toy is wearing a shirt, pick the first toy\n    return toy_patches[0].simple_query("What toy is wearing a shirt?")', 'role': 'assistant'}, {'content': '# What is behind the pole?\ndef execute_command(image)->str:', 'role': 'user'}, {'content': '    image_patch = ImagePatch(image)\n    # contains a relation (around, next to, on, near, on top of, in front of, behind, etc), so ask directly\n    return image_patch.simple_query("What is behind the pole?")', 'role': 'assistant'}, {'content': '# Are there bagels or lemons?\ndef execute_command(image)->str:', 'role': 'user'}, {'content': '    image_patch = ImagePatch(image)\n    is_bagel = image_patch.exists("bagel")\n    is_lemon = image_patch.exists("lemon")\n    return bool_to_yesno(is_bagel or is_lemon)', 'role': 'assistant'}, {'content': '# Is that blanket to the right of a pillow?\ndef execute_command(image)->str:', 'role': 'user'}, {'content': '    image_patch = ImagePatch(image)\n    blanket_patches = image_patch.find("blanket")\n    # Question assumes only one blanket patch\n    if len(blanket_patches) == 0:\n        # If no blanket is found, query the image directly\n        return image_patch.simple_query("Is that blanket to the right of a pillow?")\n    for blanket_patch in blanket_patches:\n        pillow_patches = image_patch.find("pillow")\n        for pillow_patch in pillow_patches:\n            if pillow_patch.horizontal_center > blanket_patch.horizontal_center:\n                return "yes"\n    return "no"', 'role': 'assistant'}, {'content': 'Are there men to the left of the guy that is wearing sneakers?\ndef execute_command(image)->str:', 'role': 'user'}], 'chosen': '\n    image_patch = ImagePatch(image)\n    guy_patches = image_patch.find("guy")\n    if len(guy_patches) == 0:\n        return image_patch.simple_query("Are there men to the left of the guy that is wearing sneakers?")\n    guy_patch = guy_patches[0]\n    if not guy_patch.verify_property("guy", "wearing sneakers"):\n        return "no"\n    man_patches = image_patch.find("man")\n    if len(man_patches) == 0:\n        return "no"\n    man_patch = man_patches[0]\n    if man_patch.horizontal_center < guy_patch.horizontal_center:\n        return "yes"\n    return "no"\n', 'rejected': ' \n    image_patch = ImagePatch(image)\n    man_patches_wearing_sneakers = image_patch.find("man").find("wearing sneakers")\n    for man_patch_wearing_sneakers in man_patches_wearing_sneakers:\n        men_to_the_left = image_patch.find("man").find("to the left")\n        # Question assumes only one man patch wearing sneakers\n        if len(men_to_the_left) == 0:\n            # If no man is found, query the image directly\n            return image_patch.simple_query("Are there men to the left of the guy that is wearing sneakers?")\n        for man_patch_to_the_left in men_to_the_left:\n            if man_patch_to_the_left horizontal_center < man_patch_wearing_sneakers horizontal_center:\n                return "yes"\n    return "no"'}
2025-05-24 11:46:55,991 - INFO - Dev dataset example: {'prompt': [{'content': 'You are an AI that uses a special ImagePatch class to answer questions about images.\nHere is the class definition:\n\nfrom PIL import Image\nfrom vision_functions import find_in_image, simple_qa, verify_property, best_text_match\n\ndef bool_to_yesno(bool_answer: bool)->str:\n    return "yes" if bool_answer else "no"\n\nclass ImagePatch:\n    """A Python class containing a crop of an image centered around a particular object, as well as relevant information.\n    Attributes\n    ----------\n    cropped_image : array_like\n        An array-like of the cropped image taken from the original image.\n    left : int\n        An int describing the position of the left border of the crop\'s bounding box in the original image.\n    lower : int\n        An int describing the position of the bottom border of the crop\'s bounding box in the original image.\n    right : int\n        An int describing the position of the right border of the crop\'s bounding box in the original image.\n    upper : int\n        An int describing the position of the top border of the crop\'s bounding box in the original image.\n\n    Methods\n    -------\n    find(object_name: str)->List[ImagePatch]\n        Returns a list of new ImagePatch objects containing crops of the image centered around any objects found in the image matching the object_name.\n    simple_query(question: str=None)->str\n        Returns the answer to a basic question asked about the image. If no question is provided, returns the answer to "What is this?".\n    exists(object_name: str)->bool\n        Returns True if the object specified by object_name is found in the image, and False otherwise.\n    verify_property(property: str)->bool\n        Returns True if the property is met, and False otherwise.\n    best_text_match(string1: str, string2: str)->str\n        Returns the string that best matches the image.\n    crop(left: int, lower: int, right: int, upper: int)->ImagePatch\n        Returns a new ImagePatch object containing a crop of the image at the given coordinates.\n        """\n\n    def __init__(self, image, left: int=None, lower: int=None, right: int=None, upper: int=None):\n        """Initializes an ImagePatch object by cropping the image at the given coordinates and stores the coordinates as attributes.\n        If no coordinates are provided, the image is left unmodified, and the coordinates are set to the dimensions of the image.\n        Parameters\n        -------\n        image : array_like\n            An array-like of the original image.\n        left : int\n            An int describing the position of the left border of the crop\'s bounding box in the original image.\n        lower : int\n            An int describing the position of the bottom border of the crop\'s bounding box in the original image.\n        right : int\n            An int describing the position of the right border of the crop\'s bounding box in the original image.\n        upper : int\n            An int describing the position of the top border of the crop\'s bounding box in the original image.\n\n        """\n        if left is None and right is None and upper is None and lower is None:\n            self.cropped_image = image\n            self.left = 0\n            self.lower = 0\n            self.right = image.shape[2]  # width\n            self.upper = image.shape[1]  # height\n        else:\n            self.cropped_image = image[:, lower:upper, left:right]\n            self.left = left\n            self.upper = upper\n            self.right = right\n            self.lower = lower\n\n        self.width = self.cropped_image.shape[2]\n        self.height = self.cropped_image.shape[1]\n\n        self.horizontal_center = (self.left + self.right) / 2\n        self.vertical_center = (self.lower + self.upper) / 2\n\n    def find(self, object_name: str)->List["ImagePatch"]:\n        """Returns a new ImagePatch object containing the crop of the image centered around the object specified by object_name.\n        Parameters\n        -------\n        object_name : str\n            A string describing the name of the object to be found in the image.\n        """\n        return find_in_image(self.cropped_image, object_name)\n\n    def simple_query(self, question: str=None)->str:\n        """Returns the answer to a basic question asked about the image. If no question is provided, returns the answer to "What is this?".\n        Parameters\n        -------\n        question : str\n            A string describing the question to be asked.\n\n        Examples\n        -------\n\n        >>> # Which kind of animal is not eating?\n        >>> def execute_command(image)->str:\n        >>>     image_patch = ImagePatch(image)\n        >>>     animal_patches = image_patch.find("animal")\n        >>>     for animal_patch in animal_patches:\n        >>>         if not animal_patch.verify_property("animal", "eating"):\n        >>>             return animal_patch.simple_query("What kind of animal is eating?") # crop would include eating so keep it in the query\n        >>>     # If no animal is not eating, query the image directly\n        >>>     return image_patch.simple_query("Which kind of animal is not eating?")\n\n        >>> # What is in front of the horse?\n        >>> # contains a relation (around, next to, on, near, on top of, in front of, behind, etc), so ask directly\n        >>> return image_patch.simple_query("What is in front of the horse?")\n        >>>\n        """\n        return simple_qa(self.cropped_image, question)\n\n    def exists(self, object_name: str)->bool:\n        """Returns True if the object specified by object_name is found in the image, and False otherwise.\n        Parameters\n        -------\n        object_name : str\n            A string describing the name of the object to be found in the image.\n\n        Examples\n        -------\n        >>> # Are there both cakes and gummy bears in the photo?\n        >>> def execute_command(image)->str:\n        >>>     image_patch = ImagePatch(image)\n        >>>     is_cake = image_patch.exists("cake")\n        >>>     is_gummy_bear = image_patch.exists("gummy bear")\n        >>>     return bool_to_yesno(is_cake and is_gummy_bear)\n        """\n        return len(self.find(object_name)) > 0\n\n    def verify_property(self, object_name: str, property: str)->bool:\n        """Returns True if the object possesses the property, and False otherwise.\n        Differs from \'exists\' in that it presupposes the existence of the object specified by object_name, instead checking whether the object possesses the property.\n        Parameters\n        -------\n        object_name : str\n            A string describing the name of the object to be found in the image.\n        property : str\n            A string describing the property to be checked.\n\n        Examples\n        -------\n        >>> # Do the letters have blue color?\n        >>> def execute_command(image)->str:\n        >>>     image_patch = ImagePatch(image)\n        >>>     letters_patches = image_patch.find("letters")\n        >>>     # Question assumes only one letter patch\n        >>>     if len(letters_patches) == 0:\n        >>>         # If no letters are found, query the image directly\n        >>>         return image_patch.simple_query("Do the letters have blue color?")\n        >>>     return bool_to_yesno(letters_patches[0].verify_property("letters", "blue"))\n        """\n        return verify_property(self.cropped_image, object_name, property)\n\n    def best_text_match(self, option_list: List[str]) -> str:\n        """Returns the string that best matches the image.\n        Parameters\n        -------\n        option_list : str\n            A list with the names of the different options\n        prefix : str\n            A string with the prefixes to append to the options\n\n        Examples\n        -------\n        >>> # Is the cap gold or white?\n        >>> def execute_command(image)->str:\n        >>>     image_patch = ImagePatch(image)\n        >>>     cap_patches = image_patch.find("cap")\n        >>>     # Question assumes one cap patch\n        >>>     if len(cap_patches) == 0:\n        >>>         # If no cap is found, query the image directly\n        >>>         return image_patch.simple_query("Is the cap gold or white?")\n        >>>     return cap_patches[0].best_text_match(["gold", "white"])\n        """\n        return best_text_match(self.cropped_image, option_list)\n\n    def crop(self, left: int, lower: int, right: int, upper: int)->"ImagePatch":\n        """Returns a new ImagePatch cropped from the current ImagePatch.\n        Parameters\n        -------\n        left : int\n            The leftmost pixel of the cropped image.\n        lower : int\n            The lowest pixel of the cropped image.\n        right : int\n            The rightmost pixel of the cropped image.\n        upper : int\n            The uppermost pixel of the cropped image.\n        -------\n        """\n        return ImagePatch(self.cropped_image, left, lower, right, upper)\n\n\n\nPlease use this class to answer queries about images.\nWhen writing the final solution, you typically define a function:\n\ndef execute_command(image)->str:\n    # put your logic here\nYour job is to produce the correct code in that function so that it answers the question or does the operation asked by the user.\n', 'role': 'system'}, {'content': '# Is there a backpack to the right of the man?\ndef execute_command(image)->str:', 'role': 'user'}, {'content': '    image_patch = ImagePatch(image)\n    man_patches = image_patch.find("man")\n    # Question assumes one man patch\n    if len(man_patches) == 0:\n        # If no man is found, query the image directly\n        return image_patch.simple_query("Is there a backpack to the right of the man?")\n    man_patch = man_patches[0]\n    backpack_patches = image_patch.find("backpack")\n    # Question assumes one backpack patch\n    if len(backpack_patches) == 0:\n        return "no"\n    for backpack_patch in backpack_patches:\n        if backpack_patch.horizontal_center > man_patch.horizontal_center:\n            return "yes"\n    return "no"', 'role': 'assistant'}, {'content': '# In which part is the bread, the bottom or the top?\ndef execute_command(image)->str:', 'role': 'user'}, {'content': '    image_patch = ImagePatch(image)\n    bread_patches = image_patch.find("bread")\n    # Question assumes only one bread patch\n    if len(bread_patches) == 0:\n        # If no bread is found, query the image directly\n        return image_patch.simple_query("In which part is the bread, the bottom or the top?")\n    if bread_patches[0].vertical_center < image_patch.vertical_center:\n        return "bottom"\n    else:\n        return "top"', 'role': 'assistant'}, {'content': '# What type of weather do you see in the photograph?\ndef execute_command(image)->str:', 'role': 'user'}, {'content': '    image_patch = ImagePatch(image)\n    return image_patch.simple_query("What type of weather do you see in the photograph?")', 'role': 'assistant'}, {'content': '# Who is the man staring at?\ndef execute_command(image)->str:', 'role': 'user'}, {'content': '    # asks for the predicate of a relational verb (staring at), so ask directly\n    image_patch = ImagePatch(image)\n    return image_patch.simple_query("Who is the man staring at?")', 'role': 'assistant'}, {'content': '# What toy is wearing a shirt?\ndef execute_command(image)->str:', 'role': 'user'}, {'content': '    # not a relational verb so go step by step\n    image_patch = ImagePatch(image)\n    toy_patches = image_patch.find("toy")\n    # Question assumes only one toy patch\n    if len(toy_patches) == 0:\n        # If no toy is found, query the image directly\n        return image_patch.simple_query("What toy is wearing a shirt?")\n    for toy_patch in toy_patches:\n        is_wearing_shirt = (toy_patch.simple_query("Is the toy wearing a shirt?") == "yes")\n        if is_wearing_shirt:\n            return toy_patch.simple_query("What toy is wearing a shirt?") # crop would include the shirt so keep it in the query\n    # If no toy is wearing a shirt, pick the first toy\n    return toy_patches[0].simple_query("What toy is wearing a shirt?")', 'role': 'assistant'}, {'content': '# What is behind the pole?\ndef execute_command(image)->str:', 'role': 'user'}, {'content': '    image_patch = ImagePatch(image)\n    # contains a relation (around, next to, on, near, on top of, in front of, behind, etc), so ask directly\n    return image_patch.simple_query("What is behind the pole?")', 'role': 'assistant'}, {'content': '# Are there bagels or lemons?\ndef execute_command(image)->str:', 'role': 'user'}, {'content': '    image_patch = ImagePatch(image)\n    is_bagel = image_patch.exists("bagel")\n    is_lemon = image_patch.exists("lemon")\n    return bool_to_yesno(is_bagel or is_lemon)', 'role': 'assistant'}, {'content': '# Is that blanket to the right of a pillow?\ndef execute_command(image)->str:', 'role': 'user'}, {'content': '    image_patch = ImagePatch(image)\n    blanket_patches = image_patch.find("blanket")\n    # Question assumes only one blanket patch\n    if len(blanket_patches) == 0:\n        # If no blanket is found, query the image directly\n        return image_patch.simple_query("Is that blanket to the right of a pillow?")\n    for blanket_patch in blanket_patches:\n        pillow_patches = image_patch.find("pillow")\n        for pillow_patch in pillow_patches:\n            if pillow_patch.horizontal_center > blanket_patch.horizontal_center:\n                return "yes"\n    return "no"', 'role': 'assistant'}, {'content': 'Are the white blinds to the right of a clock?\ndef execute_command(image)->str:', 'role': 'user'}], 'chosen': ' # assumes one clock and one white blind\n    image_patch = ImagePatch(image)\n    white_blinds_patches = image_patch.find("white blind")\n    for white_blind_patch in white_blinds_patches:\n        if white_blind_patch.horizontal_center > image_patch.horizontal_center:\n            return "yes"\n    return "no"', 'rejected': '\n    image_patch = ImagePatch(image)\n    white_blinds_patches = image_patch.find("white blinds")\n    # Question assumes only one white blinds patch\n    if len(white_blinds_patches) == 0:\n        # If no blanket is found, query the image directly\n        return image_patch.simple_query("Are the white blinds to the right of a clock?")\n    for white_blind_patch in white_blinds_patches:\n        clock_patches = image_patch.find("clock")\n        for clock_patch in clock_patches:\n            if clock_patch.horizontal_center > white_blind_patch.horizontal_center:\n                return "yes"\n    return "no"'}
2025-05-24 11:46:55,992 - INFO - Initializing DPOTrainer...
Extracting prompt in train dataset (num_proc=128):   0%|          | 0/52487 [00:00<?, ? examples/s]Extracting prompt in train dataset (num_proc=128):   0%|          | 48/52487 [00:00<02:23, 366.08 examples/s]Extracting prompt in train dataset (num_proc=128):   0%|          | 101/52487 [00:00<02:58, 292.90 examples/s]Extracting prompt in train dataset (num_proc=128):   0%|          | 132/52487 [00:00<05:07, 170.29 examples/s]Extracting prompt in train dataset (num_proc=128):   0%|          | 173/52487 [00:00<05:00, 174.06 examples/s]Extracting prompt in train dataset (num_proc=128):   1%|          | 323/52487 [00:01<02:04, 420.14 examples/s]Extracting prompt in train dataset (num_proc=128):   2%|‚ñè         | 947/52487 [00:01<00:31, 1632.24 examples/s]Extracting prompt in train dataset (num_proc=128):   3%|‚ñé         | 1553/52487 [00:01<00:21, 2396.40 examples/s]Extracting prompt in train dataset (num_proc=128):   4%|‚ñç         | 1972/52487 [00:01<00:21, 2314.20 examples/s]Extracting prompt in train dataset (num_proc=128):   4%|‚ñç         | 2285/52487 [00:01<00:20, 2473.04 examples/s]Extracting prompt in train dataset (num_proc=128):   5%|‚ñå         | 2803/52487 [00:01<00:16, 3041.25 examples/s]Extracting prompt in train dataset (num_proc=128):   7%|‚ñã         | 3419/52487 [00:01<00:12, 3787.20 examples/s]Extracting prompt in train dataset (num_proc=128):   8%|‚ñä         | 4148/52487 [00:01<00:10, 4668.42 examples/s]Extracting prompt in train dataset (num_proc=128):   9%|‚ñâ         | 4680/52487 [00:01<00:09, 4839.53 examples/s]Extracting prompt in train dataset (num_proc=128):  10%|‚ñâ         | 5203/52487 [00:02<00:09, 4737.65 examples/s]Extracting prompt in train dataset (num_proc=128):  11%|‚ñà         | 5716/52487 [00:02<00:10, 4636.42 examples/s]Extracting prompt in train dataset (num_proc=128):  12%|‚ñà‚ñè        | 6215/52487 [00:02<00:09, 4675.28 examples/s]Extracting prompt in train dataset (num_proc=128):  13%|‚ñà‚ñé        | 6858/52487 [00:02<00:08, 5143.26 examples/s]Extracting prompt in train dataset (num_proc=128):  14%|‚ñà‚ñç        | 7530/52487 [00:02<00:08, 5469.42 examples/s]Extracting prompt in train dataset (num_proc=128):  15%|‚ñà‚ñå        | 8094/52487 [00:02<00:08, 5478.16 examples/s]Extracting prompt in train dataset (num_proc=128):  17%|‚ñà‚ñã        | 8903/52487 [00:02<00:07, 6219.41 examples/s]Extracting prompt in train dataset (num_proc=128):  18%|‚ñà‚ñä        | 9544/52487 [00:02<00:07, 5706.37 examples/s]Extracting prompt in train dataset (num_proc=128):  19%|‚ñà‚ñâ        | 10230/52487 [00:02<00:07, 5947.82 examples/s]Extracting prompt in train dataset (num_proc=128):  21%|‚ñà‚ñà        | 10848/52487 [00:03<00:07, 5862.10 examples/s]Extracting prompt in train dataset (num_proc=128):  22%|‚ñà‚ñà‚ñè       | 11655/52487 [00:03<00:06, 6460.31 examples/s]Extracting prompt in train dataset (num_proc=128):  24%|‚ñà‚ñà‚ñé       | 12413/52487 [00:03<00:05, 6736.01 examples/s]Extracting prompt in train dataset (num_proc=128):  25%|‚ñà‚ñà‚ñç       | 13101/52487 [00:03<00:06, 5883.85 examples/s]Extracting prompt in train dataset (num_proc=128):  26%|‚ñà‚ñà‚ñå       | 13723/52487 [00:03<00:09, 4267.31 examples/s]Extracting prompt in train dataset (num_proc=128):  27%|‚ñà‚ñà‚ñã       | 14228/52487 [00:03<00:08, 4285.25 examples/s]Extracting prompt in train dataset (num_proc=128):  28%|‚ñà‚ñà‚ñä       | 14877/52487 [00:03<00:07, 4754.91 examples/s]Extracting prompt in train dataset (num_proc=128):  30%|‚ñà‚ñà‚ñâ       | 15536/52487 [00:03<00:07, 5191.71 examples/s]Extracting prompt in train dataset (num_proc=128):  31%|‚ñà‚ñà‚ñà       | 16113/52487 [00:04<00:06, 5208.57 examples/s]Extracting prompt in train dataset (num_proc=128):  32%|‚ñà‚ñà‚ñà‚ñè      | 16685/52487 [00:04<00:06, 5288.28 examples/s]Extracting prompt in train dataset (num_proc=128):  33%|‚ñà‚ñà‚ñà‚ñé      | 17263/52487 [00:04<00:06, 5395.38 examples/s]Extracting prompt in train dataset (num_proc=128):  34%|‚ñà‚ñà‚ñà‚ñç      | 17827/52487 [00:04<00:06, 5244.21 examples/s]Extracting prompt in train dataset (num_proc=128):  35%|‚ñà‚ñà‚ñà‚ñå      | 18541/52487 [00:04<00:05, 5731.72 examples/s]Extracting prompt in train dataset (num_proc=128):  36%|‚ñà‚ñà‚ñà‚ñã      | 19130/52487 [00:04<00:06, 5537.07 examples/s]Extracting prompt in train dataset (num_proc=128):  38%|‚ñà‚ñà‚ñà‚ñä      | 19735/52487 [00:04<00:05, 5666.11 examples/s]Extracting prompt in train dataset (num_proc=128):  39%|‚ñà‚ñà‚ñà‚ñâ      | 20362/52487 [00:04<00:05, 5835.82 examples/s]Extracting prompt in train dataset (num_proc=128):  40%|‚ñà‚ñà‚ñà‚ñà      | 20997/52487 [00:04<00:05, 5982.06 examples/s]Extracting prompt in train dataset (num_proc=128):  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 21659/52487 [00:05<00:05, 6162.82 examples/s]Extracting prompt in train dataset (num_proc=128):  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22445/52487 [00:05<00:04, 6574.06 examples/s]Extracting prompt in train dataset (num_proc=128):  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 23216/52487 [00:05<00:04, 6891.30 examples/s]Extracting prompt in train dataset (num_proc=128):  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23927/52487 [00:05<00:04, 6682.33 examples/s]Extracting prompt in train dataset (num_proc=128):  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24603/52487 [00:05<00:04, 5789.60 examples/s]Extracting prompt in train dataset (num_proc=128):  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 25222/52487 [00:05<00:04, 5671.35 examples/s]Extracting prompt in train dataset (num_proc=128):  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 25815/52487 [00:05<00:04, 5490.72 examples/s]Extracting prompt in train dataset (num_proc=128):  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26377/52487 [00:05<00:04, 5410.59 examples/s]Extracting prompt in train dataset (num_proc=128):  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 26945/52487 [00:05<00:04, 5170.75 examples/s]Extracting prompt in train dataset (num_proc=128):  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27562/52487 [00:06<00:04, 5397.93 examples/s]Extracting prompt in train dataset (num_proc=128):  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 28160/52487 [00:06<00:04, 5544.11 examples/s]Extracting prompt in train dataset (num_proc=128):  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 29208/52487 [00:06<00:03, 6914.02 examples/s]Extracting prompt in train dataset (num_proc=128):  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29919/52487 [00:06<00:03, 6576.38 examples/s]Extracting prompt in train dataset (num_proc=128):  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 30601/52487 [00:06<00:03, 6499.19 examples/s]Extracting prompt in train dataset (num_proc=128):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 31259/52487 [00:06<00:03, 6206.00 examples/s]Extracting prompt in train dataset (num_proc=128):  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31901/52487 [00:06<00:03, 6243.11 examples/s]Extracting prompt in train dataset (num_proc=128):  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 32540/52487 [00:06<00:03, 6220.28 examples/s]Extracting prompt in train dataset (num_proc=128):  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 33181/52487 [00:06<00:03, 6166.03 examples/s]Extracting prompt in train dataset (num_proc=128):  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33809/52487 [00:07<00:03, 6027.03 examples/s]Extracting prompt in train dataset (num_proc=128):  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 34444/52487 [00:07<00:02, 6107.72 examples/s]Extracting prompt in train dataset (num_proc=128):  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 35066/52487 [00:07<00:02, 5892.66 examples/s]Extracting prompt in train dataset (num_proc=128):  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35772/52487 [00:07<00:02, 6198.67 examples/s]Extracting prompt in train dataset (num_proc=128):  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 36417/52487 [00:07<00:02, 6241.84 examples/s]Extracting prompt in train dataset (num_proc=128):  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 37085/52487 [00:07<00:02, 6313.63 examples/s]Extracting prompt in train dataset (num_proc=128):  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 37721/52487 [00:07<00:02, 6220.86 examples/s]Extracting prompt in train dataset (num_proc=128):  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 38346/52487 [00:07<00:02, 5721.62 examples/s]Extracting prompt in train dataset (num_proc=128):  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38999/52487 [00:08<00:03, 3957.14 examples/s]Extracting prompt in train dataset (num_proc=128):  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 39507/52487 [00:08<00:03, 4189.02 examples/s]Extracting prompt in train dataset (num_proc=128):  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 40064/52487 [00:08<00:02, 4494.85 examples/s]Extracting prompt in train dataset (num_proc=128):  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 40573/52487 [00:08<00:02, 4588.65 examples/s]Extracting prompt in train dataset (num_proc=128):  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 41103/52487 [00:08<00:02, 4762.86 examples/s]Extracting prompt in train dataset (num_proc=128):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 41751/52487 [00:08<00:02, 5224.38 examples/s]Extracting prompt in train dataset (num_proc=128):  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 42318/52487 [00:08<00:01, 5331.77 examples/s]Extracting prompt in train dataset (num_proc=128):  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42922/52487 [00:08<00:01, 5526.17 examples/s]Extracting prompt in train dataset (num_proc=128):  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 43655/52487 [00:08<00:01, 6010.84 examples/s]Extracting prompt in train dataset (num_proc=128):  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 44334/52487 [00:08<00:01, 6220.05 examples/s]Extracting prompt in train dataset (num_proc=128):  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 44968/52487 [00:09<00:01, 5944.91 examples/s]Extracting prompt in train dataset (num_proc=128):  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 45581/52487 [00:09<00:01, 5995.02 examples/s]Extracting prompt in train dataset (num_proc=128):  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 46187/52487 [00:09<00:01, 5737.42 examples/s]Extracting prompt in train dataset (num_proc=128):  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 46786/52487 [00:09<00:01, 5683.90 examples/s]Extracting prompt in train dataset (num_proc=128):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 47431/52487 [00:09<00:00, 5896.43 examples/s]Extracting prompt in train dataset (num_proc=128):  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 48033/52487 [00:09<00:00, 5861.96 examples/s]Extracting prompt in train dataset (num_proc=128):  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 48641/52487 [00:09<00:00, 5821.73 examples/s]Extracting prompt in train dataset (num_proc=128):  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 49241/52487 [00:09<00:00, 5711.96 examples/s]Extracting prompt in train dataset (num_proc=128):  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 49833/52487 [00:09<00:00, 5631.56 examples/s]Extracting prompt in train dataset (num_proc=128):  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 50465/52487 [00:10<00:00, 5663.57 examples/s]Extracting prompt in train dataset (num_proc=128):  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 51115/52487 [00:10<00:00, 5900.27 examples/s]Extracting prompt in train dataset (num_proc=128):  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51811/52487 [00:10<00:00, 6029.69 examples/s]Extracting prompt in train dataset (num_proc=128): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 52477/52487 [00:10<00:00, 5961.84 examples/s]Extracting prompt in train dataset (num_proc=128): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52487/52487 [00:11<00:00, 4721.62 examples/s]
Applying chat template to train dataset (num_proc=128):   0%|          | 0/52487 [00:00<?, ? examples/s]Applying chat template to train dataset (num_proc=128):   1%|          | 411/52487 [00:00<01:16, 679.72 examples/s]Applying chat template to train dataset (num_proc=128):   2%|‚ñè         | 1233/52487 [00:00<00:27, 1890.22 examples/s]Applying chat template to train dataset (num_proc=128):   4%|‚ñç         | 2055/52487 [00:00<00:18, 2791.16 examples/s]Applying chat template to train dataset (num_proc=128):   5%|‚ñå         | 2877/52487 [00:01<00:14, 3451.90 examples/s]Applying chat template to train dataset (num_proc=128):   7%|‚ñã         | 3697/52487 [00:01<00:12, 3923.74 examples/s]Applying chat template to train dataset (num_proc=128):   9%|‚ñä         | 4517/52487 [00:01<00:11, 4226.07 examples/s]Applying chat template to train dataset (num_proc=128):  10%|‚ñà         | 5337/52487 [00:01<00:10, 4482.75 examples/s]Applying chat template to train dataset (num_proc=128):  12%|‚ñà‚ñè        | 6157/52487 [00:01<00:09, 4657.79 examples/s]Applying chat template to train dataset (num_proc=128):  13%|‚ñà‚ñé        | 6977/52487 [00:01<00:09, 4742.03 examples/s]Applying chat template to train dataset (num_proc=128):  15%|‚ñà‚ñç        | 7797/52487 [00:02<00:09, 4821.65 examples/s]Applying chat template to train dataset (num_proc=128):  16%|‚ñà‚ñã        | 8617/52487 [00:02<00:08, 4885.48 examples/s]Applying chat template to train dataset (num_proc=128):  18%|‚ñà‚ñä        | 9437/52487 [00:02<00:08, 4912.63 examples/s]Applying chat template to train dataset (num_proc=128):  20%|‚ñà‚ñâ        | 10257/52487 [00:02<00:08, 4945.60 examples/s]Applying chat template to train dataset (num_proc=128):  21%|‚ñà‚ñà        | 11077/52487 [00:02<00:08, 4948.32 examples/s]Applying chat template to train dataset (num_proc=128):  23%|‚ñà‚ñà‚ñé       | 11897/52487 [00:02<00:08, 4913.63 examples/s]Applying chat template to train dataset (num_proc=128):  25%|‚ñà‚ñà‚ñå       | 13127/52487 [00:03<00:07, 5327.16 examples/s]Applying chat template to train dataset (num_proc=128):  27%|‚ñà‚ñà‚ñã       | 13947/52487 [00:03<00:07, 5238.79 examples/s]Applying chat template to train dataset (num_proc=128):  28%|‚ñà‚ñà‚ñä       | 14767/52487 [00:03<00:07, 5077.31 examples/s]Applying chat template to train dataset (num_proc=128):  30%|‚ñà‚ñà‚ñâ       | 15587/52487 [00:03<00:07, 5028.29 examples/s]Applying chat template to train dataset (num_proc=128):  31%|‚ñà‚ñà‚ñà‚ñè      | 16407/52487 [00:03<00:08, 4378.27 examples/s]Applying chat template to train dataset (num_proc=128):  33%|‚ñà‚ñà‚ñà‚ñé      | 17227/52487 [00:04<00:07, 4535.36 examples/s]Applying chat template to train dataset (num_proc=128):  34%|‚ñà‚ñà‚ñà‚ñç      | 18047/52487 [00:04<00:07, 4609.84 examples/s]Applying chat template to train dataset (num_proc=128):  37%|‚ñà‚ñà‚ñà‚ñã      | 19277/52487 [00:04<00:07, 4741.12 examples/s]Applying chat template to train dataset (num_proc=128):  39%|‚ñà‚ñà‚ñà‚ñâ      | 20507/52487 [00:04<00:06, 4796.43 examples/s]Applying chat template to train dataset (num_proc=128):  41%|‚ñà‚ñà‚ñà‚ñà      | 21327/52487 [00:04<00:06, 4861.75 examples/s]Applying chat template to train dataset (num_proc=128):  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 22147/52487 [00:05<00:06, 4859.25 examples/s]Applying chat template to train dataset (num_proc=128):  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 23377/52487 [00:05<00:05, 4917.93 examples/s]Applying chat template to train dataset (num_proc=128):  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24607/52487 [00:05<00:05, 4938.51 examples/s]Applying chat template to train dataset (num_proc=128):  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 25427/52487 [00:05<00:05, 4940.13 examples/s]Applying chat template to train dataset (num_proc=128):  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26247/52487 [00:05<00:05, 4957.64 examples/s]Applying chat template to train dataset (num_proc=128):  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 27067/52487 [00:06<00:05, 4921.60 examples/s]Applying chat template to train dataset (num_proc=128):  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 27887/52487 [00:06<00:04, 4942.28 examples/s]Applying chat template to train dataset (num_proc=128):  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 29117/52487 [00:06<00:04, 4952.08 examples/s]Applying chat template to train dataset (num_proc=128):  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 30347/52487 [00:06<00:03, 5548.92 examples/s]Applying chat template to train dataset (num_proc=128):  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 31167/52487 [00:06<00:04, 4751.15 examples/s]Applying chat template to train dataset (num_proc=128):  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31987/52487 [00:07<00:04, 4580.47 examples/s]Applying chat template to train dataset (num_proc=128):  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 33217/52487 [00:07<00:03, 5344.71 examples/s]Applying chat template to train dataset (num_proc=128):  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 34037/52487 [00:07<00:03, 4675.14 examples/s]Applying chat template to train dataset (num_proc=128):  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34857/52487 [00:07<00:03, 4793.60 examples/s]Applying chat template to train dataset (num_proc=128):  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35677/52487 [00:07<00:04, 4158.04 examples/s]Applying chat template to train dataset (num_proc=128):  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 37317/52487 [00:08<00:02, 5518.39 examples/s]Applying chat template to train dataset (num_proc=128):  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 38137/52487 [00:08<00:02, 4841.58 examples/s]Applying chat template to train dataset (num_proc=128):  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 39367/52487 [00:08<00:02, 4852.83 examples/s]Applying chat template to train dataset (num_proc=128):  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 40187/52487 [00:08<00:02, 4900.42 examples/s]Applying chat template to train dataset (num_proc=128):  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 41417/52487 [00:08<00:02, 4936.42 examples/s]Applying chat template to train dataset (num_proc=128):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 42237/52487 [00:09<00:02, 4965.51 examples/s]Applying chat template to train dataset (num_proc=128):  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 43057/52487 [00:09<00:01, 4971.82 examples/s]Applying chat template to train dataset (num_proc=128):  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 43877/52487 [00:09<00:01, 4962.62 examples/s]Applying chat template to train dataset (num_proc=128):  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 44697/52487 [00:09<00:01, 4409.66 examples/s]Applying chat template to train dataset (num_proc=128):  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 45927/52487 [00:09<00:01, 5033.95 examples/s]Applying chat template to train dataset (num_proc=128):  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 46747/52487 [00:10<00:01, 5037.96 examples/s]Applying chat template to train dataset (num_proc=128):  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 47977/52487 [00:10<00:00, 5021.20 examples/s]Applying chat template to train dataset (num_proc=128):  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 48797/52487 [00:10<00:00, 5024.34 examples/s]Applying chat template to train dataset (num_proc=128):  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 49617/52487 [00:10<00:00, 4967.21 examples/s]Applying chat template to train dataset (num_proc=128):  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 50437/52487 [00:10<00:00, 5301.67 examples/s]Applying chat template to train dataset (num_proc=128):  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51257/52487 [00:10<00:00, 5571.28 examples/s]Applying chat template to train dataset (num_proc=128):  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 52077/52487 [00:11<00:00, 5343.59 examples/s]Applying chat template to train dataset (num_proc=128): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52487/52487 [00:11<00:00, 4676.93 examples/s]
Tokenizing train dataset (num_proc=128):   0%|          | 0/52487 [00:00<?, ? examples/s]Tokenizing train dataset (num_proc=128):   0%|          | 79/52487 [00:00<06:45, 129.35 examples/s]Tokenizing train dataset (num_proc=128):   1%|          | 404/52487 [00:00<01:25, 608.21 examples/s]Tokenizing train dataset (num_proc=128):   2%|‚ñè         | 972/52487 [00:00<00:37, 1388.36 examples/s]Tokenizing train dataset (num_proc=128):   4%|‚ñç         | 1999/52487 [00:01<00:18, 2762.23 examples/s]Tokenizing train dataset (num_proc=128):   5%|‚ñå         | 2746/52487 [00:01<00:15, 3274.04 examples/s]Tokenizing train dataset (num_proc=128):   7%|‚ñã         | 3504/52487 [00:01<00:13, 3696.05 examples/s]Tokenizing train dataset (num_proc=128):   8%|‚ñä         | 4250/52487 [00:01<00:12, 3972.10 examples/s]Tokenizing train dataset (num_proc=128):  10%|‚ñâ         | 5079/52487 [00:01<00:11, 4246.60 examples/s]Tokenizing train dataset (num_proc=128):  11%|‚ñà         | 5825/52487 [00:01<00:10, 4321.63 examples/s]Tokenizing train dataset (num_proc=128):  13%|‚ñà‚ñé        | 6869/52487 [00:02<00:09, 4900.90 examples/s]Tokenizing train dataset (num_proc=128):  15%|‚ñà‚ñç        | 7620/52487 [00:02<00:09, 4783.77 examples/s]Tokenizing train dataset (num_proc=128):  16%|‚ñà‚ñå        | 8418/52487 [00:02<00:09, 4780.00 examples/s]Tokenizing train dataset (num_proc=128):  18%|‚ñà‚ñä        | 9222/52487 [00:02<00:09, 4771.53 examples/s]Tokenizing train dataset (num_proc=128):  19%|‚ñà‚ñâ        | 10090/52487 [00:02<00:08, 4922.09 examples/s]Tokenizing train dataset (num_proc=128):  20%|‚ñà‚ñà        | 10737/52487 [00:02<00:09, 4613.62 examples/s]Tokenizing train dataset (num_proc=128):  22%|‚ñà‚ñà‚ñè       | 11600/52487 [00:03<00:08, 4764.81 examples/s]Tokenizing train dataset (num_proc=128):  24%|‚ñà‚ñà‚ñç       | 12590/52487 [00:03<00:07, 5100.40 examples/s]Tokenizing train dataset (num_proc=128):  26%|‚ñà‚ñà‚ñå       | 13422/52487 [00:03<00:07, 5088.48 examples/s]Tokenizing train dataset (num_proc=128):  27%|‚ñà‚ñà‚ñã       | 14284/52487 [00:03<00:07, 5107.95 examples/s]Tokenizing train dataset (num_proc=128):  29%|‚ñà‚ñà‚ñä       | 14980/52487 [00:03<00:07, 4806.27 examples/s]Tokenizing train dataset (num_proc=128):  30%|‚ñà‚ñà‚ñà       | 15798/52487 [00:03<00:07, 4859.45 examples/s]Tokenizing train dataset (num_proc=128):  32%|‚ñà‚ñà‚ñà‚ñè      | 16628/52487 [00:04<00:07, 4855.87 examples/s]Tokenizing train dataset (num_proc=128):  33%|‚ñà‚ñà‚ñà‚ñé      | 17577/52487 [00:04<00:06, 5117.83 examples/s]Tokenizing train dataset (num_proc=128):  35%|‚ñà‚ñà‚ñà‚ñç      | 18356/52487 [00:04<00:06, 4998.70 examples/s]Tokenizing train dataset (num_proc=128):  36%|‚ñà‚ñà‚ñà‚ñã      | 19058/52487 [00:04<00:07, 4773.47 examples/s]Tokenizing train dataset (num_proc=128):  38%|‚ñà‚ñà‚ñà‚ñä      | 19984/52487 [00:04<00:06, 5009.59 examples/s]Tokenizing train dataset (num_proc=128):  40%|‚ñà‚ñà‚ñà‚ñâ      | 20798/52487 [00:04<00:06, 4961.65 examples/s]Tokenizing train dataset (num_proc=128):  41%|‚ñà‚ñà‚ñà‚ñà      | 21640/52487 [00:05<00:06, 4986.26 examples/s]Tokenizing train dataset (num_proc=128):  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 22381/52487 [00:05<00:06, 4840.41 examples/s]Tokenizing train dataset (num_proc=128):  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 23333/52487 [00:05<00:05, 5121.16 examples/s]Tokenizing train dataset (num_proc=128):  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 23903/52487 [00:05<00:06, 4535.26 examples/s]Tokenizing train dataset (num_proc=128):  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 24794/52487 [00:05<00:05, 4783.17 examples/s]Tokenizing train dataset (num_proc=128):  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 25580/52487 [00:05<00:05, 4761.29 examples/s]Tokenizing train dataset (num_proc=128):  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 26155/52487 [00:06<00:05, 4423.34 examples/s]Tokenizing train dataset (num_proc=128):  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 26848/52487 [00:06<00:06, 4158.26 examples/s]Tokenizing train dataset (num_proc=128):  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 27446/52487 [00:06<00:05, 4490.45 examples/s]Tokenizing train dataset (num_proc=128):  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 28221/52487 [00:06<00:05, 4534.56 examples/s]Tokenizing train dataset (num_proc=128):  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 28937/52487 [00:06<00:05, 4454.88 examples/s]Tokenizing train dataset (num_proc=128):  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 29607/52487 [00:06<00:05, 4312.28 examples/s]Tokenizing train dataset (num_proc=128):  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 30275/52487 [00:07<00:05, 4117.37 examples/s]Tokenizing train dataset (num_proc=128):  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 31138/52487 [00:07<00:04, 4429.79 examples/s]Tokenizing train dataset (num_proc=128):  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 31840/52487 [00:07<00:04, 4345.09 examples/s]Tokenizing train dataset (num_proc=128):  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 32689/52487 [00:07<00:04, 4551.75 examples/s]Tokenizing train dataset (num_proc=128):  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 33442/52487 [00:07<00:04, 4538.07 examples/s]Tokenizing train dataset (num_proc=128):  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 33929/52487 [00:07<00:04, 4605.02 examples/s]Tokenizing train dataset (num_proc=128):  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 34890/52487 [00:08<00:03, 4622.20 examples/s]Tokenizing train dataset (num_proc=128):  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 35775/52487 [00:08<00:03, 4795.76 examples/s]Tokenizing train dataset (num_proc=128):  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 36441/52487 [00:08<00:03, 4531.82 examples/s]Tokenizing train dataset (num_proc=128):  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 37036/52487 [00:08<00:03, 4226.67 examples/s]Tokenizing train dataset (num_proc=128):  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 38052/52487 [00:08<00:03, 4805.08 examples/s]Tokenizing train dataset (num_proc=128):  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 38968/52487 [00:08<00:02, 5025.19 examples/s]Tokenizing train dataset (num_proc=128):  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 39753/52487 [00:09<00:02, 4930.35 examples/s]Tokenizing train dataset (num_proc=128):  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 40474/52487 [00:09<00:02, 4761.42 examples/s]Tokenizing train dataset (num_proc=128):  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 41333/52487 [00:09<00:02, 4898.49 examples/s]Tokenizing train dataset (num_proc=128):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 42116/52487 [00:09<00:02, 4842.90 examples/s]Tokenizing train dataset (num_proc=128):  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 42967/52487 [00:09<00:01, 4944.70 examples/s]Tokenizing train dataset (num_proc=128):  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 43750/52487 [00:09<00:01, 4842.81 examples/s]Tokenizing train dataset (num_proc=128):  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 44375/52487 [00:10<00:01, 4533.01 examples/s]Tokenizing train dataset (num_proc=128):  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 45089/52487 [00:10<00:01, 4425.42 examples/s]Tokenizing train dataset (num_proc=128):  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 46153/52487 [00:10<00:01, 5005.52 examples/s]Tokenizing train dataset (num_proc=128):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 47037/52487 [00:10<00:01, 5088.98 examples/s]Tokenizing train dataset (num_proc=128):  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 47662/52487 [00:10<00:01, 4691.44 examples/s]Tokenizing train dataset (num_proc=128):  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 48821/52487 [00:10<00:00, 5960.78 examples/s]Tokenizing train dataset (num_proc=128):  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 49516/52487 [00:11<00:00, 5184.78 examples/s]Tokenizing train dataset (num_proc=128):  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 50090/52487 [00:11<00:00, 5263.31 examples/s]Tokenizing train dataset (num_proc=128):  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 50778/52487 [00:11<00:00, 5544.23 examples/s]Tokenizing train dataset (num_proc=128):  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51422/52487 [00:11<00:00, 5140.55 examples/s]Tokenizing train dataset (num_proc=128):  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 51705/52487 [00:11<00:00, 4409.76 examples/s]
multiprocess.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 3544, in _map_single
    writer.write(example)
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_writer.py", line 500, in write
    self.write_examples_on_file()
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_writer.py", line 458, in write_examples_on_file
    self.write_batch(batch_examples=batch_examples)
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_writer.py", line 568, in write_batch
    arrays.append(pa.array(typed_sequence))
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/array.pxi", line 252, in pyarrow.lib.array
  File "pyarrow/array.pxi", line 114, in pyarrow.lib._handle_arrow_array_protocol
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_writer.py", line 208, in __arrow_array__
    out = cast_array_to_feature(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 1797, in wrapper
    return func(array, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 2063, in cast_array_to_feature
    casted_array_values = _c(array.values, feature.feature)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 1797, in wrapper
    return func(array, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 2098, in cast_array_to_feature
    return array_cast(
           ^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 1797, in wrapper
    return func(array, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 1948, in array_cast
    raise TypeError(f"Couldn't cast array of type {_short_str(array.type)} to {_short_str(pa_type)}")
TypeError: Couldn't cast array of type int64 to null

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
                    ^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/utils/py_utils.py", line 678, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 3593, in _map_single
    writer.finalize()
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_writer.py", line 599, in finalize
    self.write_examples_on_file()
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_writer.py", line 458, in write_examples_on_file
    self.write_batch(batch_examples=batch_examples)
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_writer.py", line 568, in write_batch
    arrays.append(pa.array(typed_sequence))
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/array.pxi", line 252, in pyarrow.lib.array
  File "pyarrow/array.pxi", line 114, in pyarrow.lib._handle_arrow_array_protocol
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_writer.py", line 208, in __arrow_array__
    out = cast_array_to_feature(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 1797, in wrapper
    return func(array, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 2063, in cast_array_to_feature
    casted_array_values = _c(array.values, feature.feature)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 1797, in wrapper
    return func(array, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 2098, in cast_array_to_feature
    return array_cast(
           ^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 1797, in wrapper
    return func(array, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 1948, in array_cast
    raise TypeError(f"Couldn't cast array of type {_short_str(array.type)} to {_short_str(pa_type)}")
TypeError: Couldn't cast array of type int64 to null
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/sorgin1/users/jbarrutia006/viper/scripts/dpo/dpotrain.py", line 225, in <module>
    train_dpo(args)
  File "/sorgin1/users/jbarrutia006/viper/scripts/dpo/dpotrain.py", line 171, in train_dpo
    trainer = DPOTrainer(
              ^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/unsloth/trainer.py", line 203, in new_init
    original_init(self, *args, **kwargs)
  File "/sorgin1/users/jbarrutia006/viper/unsloth_compiled_cache/UnslothDPOTrainer.py", line 2067, in __init__
    super().__init__(
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 165, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/viper/unsloth_compiled_cache/UnslothDPOTrainer.py", line 784, in __init__
    train_dataset = self._prepare_dataset(train_dataset, processing_class, args, "train")
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/viper/unsloth_compiled_cache/UnslothDPOTrainer.py", line 884, in _prepare_dataset
    dataset = dataset.map(
              ^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 602, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 567, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 3259, in map
    for rank, done, content in iflatmap_unordered(
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/utils/py_utils.py", line 718, in iflatmap_unordered
    [async_result.get(timeout=0.05) for async_result in async_results]
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/utils/py_utils.py", line 718, in <listcomp>
    [async_result.get(timeout=0.05) for async_result in async_results]
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/multiprocess/pool.py", line 774, in get
    raise self._value
TypeError: Couldn't cast array of type int64 to null
multiprocess.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 3544, in _map_single
    writer.write(example)
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_writer.py", line 500, in write
    self.write_examples_on_file()
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_writer.py", line 458, in write_examples_on_file
    self.write_batch(batch_examples=batch_examples)
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_writer.py", line 568, in write_batch
    arrays.append(pa.array(typed_sequence))
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/array.pxi", line 252, in pyarrow.lib.array
  File "pyarrow/array.pxi", line 114, in pyarrow.lib._handle_arrow_array_protocol
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_writer.py", line 208, in __arrow_array__
    out = cast_array_to_feature(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 1797, in wrapper
    return func(array, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 2063, in cast_array_to_feature
    casted_array_values = _c(array.values, feature.feature)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 1797, in wrapper
    return func(array, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 2098, in cast_array_to_feature
    return array_cast(
           ^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 1797, in wrapper
    return func(array, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 1948, in array_cast
    raise TypeError(f"Couldn't cast array of type {_short_str(array.type)} to {_short_str(pa_type)}")
TypeError: Couldn't cast array of type int64 to null

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/multiprocess/pool.py", line 125, in worker
    result = (True, func(*args, **kwds))
                    ^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/utils/py_utils.py", line 678, in _write_generator_to_queue
    for i, result in enumerate(func(**kwargs)):
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 3593, in _map_single
    writer.finalize()
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_writer.py", line 599, in finalize
    self.write_examples_on_file()
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_writer.py", line 458, in write_examples_on_file
    self.write_batch(batch_examples=batch_examples)
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_writer.py", line 568, in write_batch
    arrays.append(pa.array(typed_sequence))
                  ^^^^^^^^^^^^^^^^^^^^^^^^
  File "pyarrow/array.pxi", line 252, in pyarrow.lib.array
  File "pyarrow/array.pxi", line 114, in pyarrow.lib._handle_arrow_array_protocol
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_writer.py", line 208, in __arrow_array__
    out = cast_array_to_feature(
          ^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 1797, in wrapper
    return func(array, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 2063, in cast_array_to_feature
    casted_array_values = _c(array.values, feature.feature)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 1797, in wrapper
    return func(array, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 2098, in cast_array_to_feature
    return array_cast(
           ^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 1797, in wrapper
    return func(array, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/table.py", line 1948, in array_cast
    raise TypeError(f"Couldn't cast array of type {_short_str(array.type)} to {_short_str(pa_type)}")
TypeError: Couldn't cast array of type int64 to null
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/sorgin1/users/jbarrutia006/viper/scripts/dpo/dpotrain.py", line 225, in <module>
    train_dpo(args)
  File "/sorgin1/users/jbarrutia006/viper/scripts/dpo/dpotrain.py", line 171, in train_dpo
    trainer = DPOTrainer(
              ^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/unsloth/trainer.py", line 203, in new_init
    original_init(self, *args, **kwargs)
  File "/sorgin1/users/jbarrutia006/viper/unsloth_compiled_cache/UnslothDPOTrainer.py", line 2067, in __init__
    super().__init__(
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 165, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/viper/unsloth_compiled_cache/UnslothDPOTrainer.py", line 784, in __init__
    train_dataset = self._prepare_dataset(train_dataset, processing_class, args, "train")
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/viper/unsloth_compiled_cache/UnslothDPOTrainer.py", line 884, in _prepare_dataset
    dataset = dataset.map(
              ^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 602, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 567, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/arrow_dataset.py", line 3259, in map
    for rank, done, content in iflatmap_unordered(
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/utils/py_utils.py", line 718, in iflatmap_unordered
    [async_result.get(timeout=0.05) for async_result in async_results]
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/datasets/utils/py_utils.py", line 718, in <listcomp>
    [async_result.get(timeout=0.05) for async_result in async_results]
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/multiprocess/pool.py", line 774, in get
    raise self._value
TypeError: Couldn't cast array of type int64 to null
srun: error: localhost: task 0: Exited with exit code 1
