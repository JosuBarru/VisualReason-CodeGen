ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
INFO 05-25 11:53:37 __init__.py:183] Automatically detected platform cuda.
==((====))==  Unsloth 2025.3.14: Fast Llama patching. Transformers: 4.48.2. vLLM: 0.7.1.
   \\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.325 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.5.1+cu121. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
meta-llama/Meta-Llama-3.1-8B-Instruct does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mFirst Llama instruct. Dataset: all, bsize 32, lr 1e-4, [0m at: [34mhttps://wandb.ai/jbarrutia006-upv-ehu/viperDPO/runs/9ewazlo0[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250525_115340-9ewazlo0/logs[0m
