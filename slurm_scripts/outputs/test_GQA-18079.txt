2025-06-05 16:25:37,852 - INFO - {'multiprocessing': False, 'path_pretrained_models': './pretrained_models', 'execute_code': False, 'dataset': {'dataset_name': 'GQA', 'data_path': './data/gqa', 'split': 'testdev', 'max_samples': None, 'batch_size': 8, 'start_sample': 0, 'testing': False}, 'load_models': {'maskrcnn': False, 'clip': False, 'glip': False, 'owlvit': False, 'tcl': False, 'gpt3_qa': False, 'gpt3_general': False, 'depth': False, 'blip': False, 'saliency': False, 'xvlm': False, 'codex': False, 'codellama': True, 'codellama_Q': False, 'llm_query': False, 'llm_guess': False, 'gpt3_list': False, 'qa': False, 'guess': False, 'gpt3_guess': False}, 'detect_thresholds': {'glip': 0.5, 'maskrcnn': 0.8, 'owlvit': 0.1}, 'ratio_box_area_to_image_area': 0.0, 'crop_larger_margin': True, 'verify_property': {'model': 'xvlm', 'thresh_clip': 0.6, 'thresh_tcl': 0.25, 'thresh_xvlm': 0.6}, 'best_match_model': 'xvlm', 'gpt3': {'n_votes': 1, 'qa_prompt': './prompts/gpt3/gpt3_qa.txt', 'guess_prompt': './prompts/gpt3/gpt3_process_guess.txt', 'temperature': 0.0, 'model': 'text-davinci-003'}, 'codex': {'temperature': 0, 'best_of': 1, 'max_tokens': 512, 'prompt': './prompts/benchmarks/gqa.prompt', 'model': 'codellama', 'extra_context': None, 'model_name': 'codellama/CodeLlama-7b-Instruct-hf', 'adapter': None}, 'save': False, 'save_new_results': True, 'save_codex': True, 'results_dir': './results/gqa/codex_results/', 'use_cache': True, 'clear_cache': True, 'log_every': 20, 'wandb': False, 'blip_half_precision': False, 'blip_v2_model_type': 'blip2-flan-t5-xl', 'glip_model_type': 'large', 'use_fixed_code': False, 'fixed_code_file': './prompts/fixed_code/blip2.prompt', 'cognition': {'is_setted': False}, 'use_cached_codex': False, 'use_cache_codex': False}
2025-06-05 16:25:40,747 - INFO - Starting main
SELECTED CONFIG FILES: gqa/general_config,config_codellama,gqa/save_codex,gqa/testdev
LOADING MODEL: DISABLED
INFO 06-05 16:25:44 __init__.py:183] Automatically detected platform cuda.
2025-06-05 16:25:45,898 - INFO - Using dtype=bfloat16 based on compute capability=8.0
INFO 06-05 16:25:56 config.py:526] This model supports multiple tasks: {'score', 'embed', 'reward', 'classify', 'generate'}. Defaulting to 'generate'.
INFO 06-05 16:25:56 llm_engine.py:232] Initializing a V0 LLM engine (v0.7.1) with config: model='codellama/CodeLlama-7b-Instruct-hf', speculative_config=None, tokenizer='codellama/CodeLlama-7b-Instruct-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=16384, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=codellama/CodeLlama-7b-Instruct-hf, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 06-05 16:25:57 cuda.py:235] Using Flash Attention backend.
INFO 06-05 16:25:58 model_runner.py:1111] Starting to load model codellama/CodeLlama-7b-Instruct-hf...
INFO 06-05 16:25:58 weight_utils.py:251] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.14it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.29it/s]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.26it/s]

INFO 06-05 16:26:01 model_runner.py:1116] Loading model weights took 12.5562 GB
INFO 06-05 16:26:02 worker.py:266] Memory profiling takes 1.38 seconds
INFO 06-05 16:26:02 worker.py:266] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.90) = 71.24GiB
INFO 06-05 16:26:02 worker.py:266] model weights take 12.56GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 57.20GiB.
INFO 06-05 16:26:02 executor_base.py:108] # CUDA blocks: 7321, # CPU blocks: 512
INFO 06-05 16:26:02 executor_base.py:113] Maximum concurrency for 16384 tokens per request: 7.15x
INFO 06-05 16:26:04 model_runner.py:1435] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:16,  2.12it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:00<00:15,  2.15it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:14,  2.17it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:01<00:14,  2.17it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:13,  2.18it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:02<00:13,  2.19it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:03<00:12,  2.19it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:03<00:12,  2.19it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:04<00:11,  2.20it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:04<00:11,  2.21it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:05<00:11,  2.18it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:05<00:10,  2.19it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:05<00:10,  2.20it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:06<00:09,  2.20it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:06<00:09,  2.20it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:07<00:08,  2.21it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:07<00:08,  2.21it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:08<00:07,  2.23it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:08<00:07,  2.26it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:09<00:06,  2.20it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:09<00:06,  2.23it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:09<00:05,  2.26it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:10<00:05,  2.27it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:10<00:04,  2.29it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:11<00:04,  2.31it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:11<00:03,  2.32it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:12<00:03,  2.32it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:12<00:03,  2.33it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:12<00:02,  2.34it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:13<00:02,  2.35it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:13<00:01,  2.35it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:14<00:01,  2.34it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:14<00:00,  2.35it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:15<00:00,  2.35it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.33it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.25it/s]
INFO 06-05 16:26:19 model_runner.py:1563] Graph capturing finished in 16 secs, took 0.85 GiB
INFO 06-05 16:26:19 llm_engine.py:429] init engine (profile, create kv cache, warmup model) took 18.56 seconds
2025-06-05 16:26:19,614 - INFO - Models successfully loaded
2025-06-05 16:26:19,614 - WARNING - [Memory(location=cache/joblib)]: Flushing completely the cache
2025-06-05 16:26:20,080 - INFO - Dataset loaded
modelo: <class 'vision_models.codellama'> , proceso:  codellama
{'codellama': <function make_fn.<locals>._function at 0x7effc01b80e0>}
  0%|                                                                      | 0/1573 [00:00<?, ?it/s]2025-06-05 16:26:20,199 - INFO - Not using adapter

Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:  12%|█▎        | 1/8 [00:13<01:31, 13.14s/it, est. speed input: 262.84 toks/s, output: 38.97 toks/s][AProcessed prompts: 100%|██████████| 8/8 [00:13<00:00,  1.64s/it, est. speed input: 2105.67 toks/s, output: 311.77 toks/s]
/sorgin1/users/jbarrutia006/viper/src/main_project_slurm.py:280: UserWarning: Not executing code! This is only generating the code. We set the flag 'execute_code' to False by default, because executing code generated by a language model can be dangerous. Set the flag 'execute_code' to True if you want to execute it.
  warnings.warn("Not executing code! This is only generating the code. We set the flag "
  0%|                                                            | 1/1573 [00:13<5:48:53, 13.32s/it]2025-06-05 16:26:33,500 - INFO - Not using adapter

Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:  12%|█▎        | 1/8 [00:13<01:31, 13.09s/it, est. speed input: 263.96 toks/s, output: 39.12 toks/s][AProcessed prompts: 100%|██████████| 8/8 [00:13<00:00,  1.64s/it, est. speed input: 2112.93 toks/s, output: 312.92 toks/s]
  0%|                                                            | 2/1573 [00:26<5:47:26, 13.27s/it]2025-06-05 16:26:46,704 - INFO - Not using adapter

Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:  12%|█▎        | 1/8 [00:13<01:31, 13.11s/it, est. speed input: 263.85 toks/s, output: 39.07 toks/s][AProcessed prompts: 100%|██████████| 8/8 [00:13<00:00,  1.64s/it, est. speed input: 2111.27 toks/s, output: 312.52 toks/s]
  0%|1                                                           | 3/1573 [00:39<5:46:37, 13.25s/it]2025-06-05 16:26:59,911 - INFO - Not using adapter

Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:  12%|█▎        | 1/8 [00:13<01:31, 13.11s/it, est. speed input: 263.63 toks/s, output: 39.06 toks/s][AProcessed prompts: 100%|██████████| 8/8 [00:13<00:00,  1.64s/it, est. speed input: 2111.67 toks/s, output: 312.44 toks/s]
  0%|1                                                           | 4/1573 [00:52<5:46:04, 13.23s/it]2025-06-05 16:27:13,148 - INFO - Not using adapter

Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:  12%|█▎        | 1/8 [00:13<01:31, 13.11s/it, est. speed input: 263.98 toks/s, output: 39.06 toks/s][AProcessed prompts: 100%|██████████| 8/8 [00:13<00:00,  1.64s/it, est. speed input: 2110.98 toks/s, output: 312.49 toks/s]
  0%|1                                                           | 5/1573 [01:06<5:45:49, 13.23s/it]2025-06-05 16:27:26,352 - INFO - Not using adapter

Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:  12%|█▎        | 1/8 [00:13<01:31, 13.10s/it, est. speed input: 264.35 toks/s, output: 39.08 toks/s][AProcessed prompts: 100%|██████████| 8/8 [00:13<00:00,  1.64s/it, est. speed input: 2111.73 toks/s, output: 312.66 toks/s]
  0%|2                                                           | 6/1573 [01:19<5:45:15, 13.22s/it]2025-06-05 16:27:39,571 - INFO - Not using adapter

Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:  12%|█▎        | 1/8 [00:13<01:31, 13.10s/it, est. speed input: 263.80 toks/s, output: 39.08 toks/s][AProcessed prompts: 100%|██████████| 8/8 [00:13<00:00,  1.64s/it, est. speed input: 2111.92 toks/s, output: 312.64 toks/s]
  0%|2                                                           | 7/1573 [01:32<5:45:03, 13.22s/it]2025-06-05 16:27:52,771 - INFO - Not using adapter

srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 18079 ON localhost CANCELLED AT 2025-06-05T16:28:03 ***
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][Aslurmstepd: error: *** STEP 18079.0 ON localhost CANCELLED AT 2025-06-05T16:28:03 ***
