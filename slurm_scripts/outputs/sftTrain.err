wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbarrutia006 (jbarrutia006-upv-ehu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /sorgin1/users/jbarrutia006/viper/wandb/run-20250530_082716-sxrqhtm4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run codellama
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jbarrutia006-upv-ehu/viperSFT
wandb: üöÄ View run at https://wandb.ai/jbarrutia006-upv-ehu/viperSFT/runs/sxrqhtm4
2025-05-30 08:27:17,552 - INFO - Results will be saved to: ./sft_trained_models/05-30_08-27-17
2025-05-30 08:27:17,552 - INFO - Loading model and tokenizer...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [02:16<02:16, 136.35s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [03:18<00:00, 92.52s/it] Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [03:18<00:00, 99.09s/it]
Unsloth 2025.3.14 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
2025-05-30 08:30:47,301 - INFO - Loading SFT train and dev datasets...
Formateando train SFT:   0%|          | 0/7824 [00:00<?, ? examples/s]Formateando train SFT:  13%|‚ñà‚ñé        | 1000/7824 [00:00<00:01, 5186.48 examples/s]Formateando train SFT:  26%|‚ñà‚ñà‚ñå       | 2000/7824 [00:00<00:00, 6483.00 examples/s]Formateando train SFT:  38%|‚ñà‚ñà‚ñà‚ñä      | 3000/7824 [00:00<00:00, 7073.58 examples/s]Formateando train SFT:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4000/7824 [00:00<00:00, 7429.96 examples/s]Formateando train SFT:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 5000/7824 [00:00<00:00, 7581.81 examples/s]Formateando train SFT:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 6000/7824 [00:00<00:00, 7709.72 examples/s]Formateando train SFT:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 7000/7824 [00:00<00:00, 7803.07 examples/s]Formateando train SFT: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7824/7824 [00:01<00:00, 7853.13 examples/s]Formateando train SFT: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7824/7824 [00:02<00:00, 3631.42 examples/s]
Formateando dev SFT:   0%|          | 0/1000 [00:00<?, ? examples/s]Formateando dev SFT: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 7041.58 examples/s]Formateando dev SFT: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 3264.00 examples/s]
/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
Unsloth: Tokenizing ["text"] (num_proc=128):   0%|          | 0/7824 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   1%|          | 62/7824 [00:00<01:20, 96.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   3%|‚ñé         | 248/7824 [00:00<00:19, 397.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   5%|‚ñç         | 372/7824 [00:00<00:14, 515.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   8%|‚ñä         | 620/7824 [00:01<00:07, 908.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  10%|‚ñà         | 806/7824 [00:01<00:08, 791.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  12%|‚ñà‚ñè        | 930/7824 [00:01<00:08, 821.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  15%|‚ñà‚ñå        | 1175/7824 [00:01<00:05, 1143.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  17%|‚ñà‚ñã        | 1358/7824 [00:01<00:05, 1143.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  20%|‚ñà‚ñâ        | 1541/7824 [00:01<00:05, 1137.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  22%|‚ñà‚ñà‚ñè       | 1724/7824 [00:02<00:05, 1113.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  24%|‚ñà‚ñà‚ñç       | 1907/7824 [00:02<00:06, 910.84 examples/s] Unsloth: Tokenizing ["text"] (num_proc=128):  26%|‚ñà‚ñà‚ñå       | 2029/7824 [00:02<00:06, 895.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  27%|‚ñà‚ñà‚ñã       | 2151/7824 [00:02<00:06, 927.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  31%|‚ñà‚ñà‚ñà       | 2395/7824 [00:02<00:04, 1202.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  33%|‚ñà‚ñà‚ñà‚ñé      | 2578/7824 [00:02<00:04, 1272.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  36%|‚ñà‚ñà‚ñà‚ñå      | 2822/7824 [00:02<00:03, 1351.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  39%|‚ñà‚ñà‚ñà‚ñâ      | 3066/7824 [00:03<00:03, 1566.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 3249/7824 [00:03<00:02, 1585.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 3432/7824 [00:03<00:02, 1528.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 3615/7824 [00:03<00:02, 1531.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3920/7824 [00:03<00:02, 1796.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 4164/7824 [00:03<00:01, 1941.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4408/7824 [00:03<00:01, 1874.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 4652/7824 [00:03<00:01, 1847.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 5018/7824 [00:04<00:01, 2240.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 5323/7824 [00:04<00:01, 2340.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 5811/7824 [00:04<00:00, 2946.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 6116/7824 [00:04<00:00, 2609.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 6421/7824 [00:04<00:00, 2410.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 6787/7824 [00:04<00:00, 2582.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 7153/7824 [00:04<00:00, 2837.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 7458/7824 [00:04<00:00, 2535.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 7763/7824 [00:05<00:00, 2266.30 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7824/7824 [00:05<00:00, 1306.16 examples/s]
Unsloth: Tokenizing ["text"] (num_proc=128):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   1%|          | 8/1000 [00:00<00:32, 30.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   4%|‚ñç         | 40/1000 [00:00<00:07, 132.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   6%|‚ñã         | 64/1000 [00:00<00:05, 167.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  10%|‚ñâ         | 96/1000 [00:00<00:04, 215.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  13%|‚ñà‚ñé        | 128/1000 [00:00<00:03, 248.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  16%|‚ñà‚ñå        | 160/1000 [00:00<00:03, 268.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  21%|‚ñà‚ñà        | 208/1000 [00:00<00:02, 317.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  25%|‚ñà‚ñà‚ñç       | 248/1000 [00:01<00:02, 298.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  29%|‚ñà‚ñà‚ñâ       | 288/1000 [00:01<00:02, 322.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  33%|‚ñà‚ñà‚ñà‚ñé      | 328/1000 [00:01<00:02, 328.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  37%|‚ñà‚ñà‚ñà‚ñã      | 368/1000 [00:01<00:01, 344.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  41%|‚ñà‚ñà‚ñà‚ñà      | 408/1000 [00:01<00:01, 356.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 448/1000 [00:01<00:01, 351.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 504/1000 [00:01<00:01, 389.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 544/1000 [00:01<00:01, 389.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 592/1000 [00:01<00:00, 414.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 640/1000 [00:02<00:00, 432.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 688/1000 [00:02<00:00, 354.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 728/1000 [00:02<00:00, 343.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 832/1000 [00:02<00:00, 514.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 937/1000 [00:02<00:00, 646.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:02<00:00, 371.12 examples/s]
2025-05-30 08:31:11,516 - WARNING - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-05-30 08:31:11,534 - INFO - Performing an initial evaluation on the dev_sft dataset...
  0%|          | 0/250 [00:00<?, ?it/s]  1%|          | 2/250 [00:01<02:43,  1.52it/s]  1%|          | 3/250 [00:02<04:26,  1.08s/it]  2%|‚ñè         | 4/250 [00:04<04:51,  1.19s/it]  2%|‚ñè         | 5/250 [00:05<05:25,  1.33s/it]  2%|‚ñè         | 6/250 [00:07<05:23,  1.33s/it]  3%|‚ñé         | 7/250 [00:08<05:45,  1.42s/it]  3%|‚ñé         | 8/250 [00:10<05:42,  1.41s/it]  4%|‚ñé         | 9/250 [00:11<05:53,  1.47s/it]  4%|‚ñç         | 10/250 [00:13<05:48,  1.45s/it]  4%|‚ñç         | 11/250 [00:15<06:09,  1.55s/it]  5%|‚ñç         | 12/250 [00:16<05:52,  1.48s/it]  5%|‚ñå         | 13/250 [00:18<06:05,  1.54s/it]  6%|‚ñå         | 14/250 [00:19<05:48,  1.48s/it]  6%|‚ñå         | 15/250 [00:20<05:55,  1.51s/it]  6%|‚ñã         | 16/250 [00:22<05:39,  1.45s/it]  7%|‚ñã         | 17/250 [00:23<05:49,  1.50s/it]  7%|‚ñã         | 18/250 [00:25<05:42,  1.48s/it]  8%|‚ñä         | 19/250 [00:26<05:49,  1.51s/it]  8%|‚ñä         | 20/250 [00:28<05:33,  1.45s/it]  8%|‚ñä         | 21/250 [00:29<05:43,  1.50s/it]  9%|‚ñâ         | 22/250 [00:31<05:33,  1.46s/it]  9%|‚ñâ         | 23/250 [00:32<05:47,  1.53s/it] 10%|‚ñâ         | 24/250 [00:34<05:32,  1.47s/it] 10%|‚ñà         | 25/250 [00:35<05:40,  1.51s/it] 10%|‚ñà         | 26/250 [00:37<05:26,  1.46s/it] 11%|‚ñà         | 27/250 [00:38<05:35,  1.51s/it] 11%|‚ñà         | 28/250 [00:40<05:22,  1.45s/it] 12%|‚ñà‚ñè        | 29/250 [00:42<06:00,  1.63s/it] 12%|‚ñà‚ñè        | 30/250 [00:43<05:40,  1.55s/it] 12%|‚ñà‚ñè        | 31/250 [00:45<05:43,  1.57s/it] 13%|‚ñà‚ñé        | 32/250 [00:46<05:28,  1.51s/it] 13%|‚ñà‚ñé        | 33/250 [00:48<05:36,  1.55s/it] 14%|‚ñà‚ñé        | 34/250 [00:49<05:19,  1.48s/it] 14%|‚ñà‚ñç        | 35/250 [00:51<05:32,  1.55s/it] 14%|‚ñà‚ñç        | 36/250 [00:52<05:16,  1.48s/it] 15%|‚ñà‚ñç        | 37/250 [00:54<05:26,  1.53s/it] 15%|‚ñà‚ñå        | 38/250 [00:55<05:11,  1.47s/it] 16%|‚ñà‚ñå        | 39/250 [00:57<05:21,  1.53s/it] 16%|‚ñà‚ñå        | 40/250 [00:58<05:08,  1.47s/it] 16%|‚ñà‚ñã        | 41/250 [01:00<05:17,  1.52s/it] 17%|‚ñà‚ñã        | 42/250 [01:01<05:07,  1.48s/it] 17%|‚ñà‚ñã        | 43/250 [01:03<05:17,  1.53s/it] 18%|‚ñà‚ñä        | 44/250 [01:04<05:02,  1.47s/it] 18%|‚ñà‚ñä        | 45/250 [01:06<05:10,  1.51s/it] 18%|‚ñà‚ñä        | 46/250 [01:07<05:00,  1.47s/it] 19%|‚ñà‚ñâ        | 47/250 [01:09<05:07,  1.52s/it] 19%|‚ñà‚ñâ        | 48/250 [01:10<04:57,  1.47s/it] 20%|‚ñà‚ñâ        | 49/250 [01:12<05:03,  1.51s/it] 20%|‚ñà‚ñà        | 50/250 [01:13<04:53,  1.47s/it] 20%|‚ñà‚ñà        | 51/250 [01:15<05:09,  1.55s/it] 21%|‚ñà‚ñà        | 52/250 [01:16<04:53,  1.48s/it] 21%|‚ñà‚ñà        | 53/250 [01:18<05:07,  1.56s/it] 22%|‚ñà‚ñà‚ñè       | 54/250 [01:19<04:53,  1.50s/it] 22%|‚ñà‚ñà‚ñè       | 55/250 [01:21<05:00,  1.54s/it] 22%|‚ñà‚ñà‚ñè       | 56/250 [01:22<04:49,  1.49s/it] 23%|‚ñà‚ñà‚ñé       | 57/250 [01:24<04:59,  1.55s/it] 23%|‚ñà‚ñà‚ñé       | 58/250 [01:25<04:45,  1.49s/it] 24%|‚ñà‚ñà‚ñé       | 59/250 [01:27<04:54,  1.54s/it] 24%|‚ñà‚ñà‚ñç       | 60/250 [01:28<04:40,  1.48s/it] 24%|‚ñà‚ñà‚ñç       | 61/250 [01:30<04:51,  1.54s/it] 25%|‚ñà‚ñà‚ñç       | 62/250 [01:31<04:36,  1.47s/it] 25%|‚ñà‚ñà‚ñå       | 63/250 [01:33<04:47,  1.54s/it] 26%|‚ñà‚ñà‚ñå       | 64/250 [01:34<04:36,  1.49s/it] 26%|‚ñà‚ñà‚ñå       | 65/250 [01:36<04:42,  1.53s/it] 26%|‚ñà‚ñà‚ñã       | 66/250 [01:37<04:29,  1.47s/it] 27%|‚ñà‚ñà‚ñã       | 67/250 [01:39<04:45,  1.56s/it] 27%|‚ñà‚ñà‚ñã       | 68/250 [01:40<04:35,  1.51s/it] 28%|‚ñà‚ñà‚ñä       | 69/250 [01:42<04:41,  1.56s/it] 28%|‚ñà‚ñà‚ñä       | 70/250 [01:43<04:26,  1.48s/it] 28%|‚ñà‚ñà‚ñä       | 71/250 [01:45<04:35,  1.54s/it] 29%|‚ñà‚ñà‚ñâ       | 72/250 [01:46<04:24,  1.49s/it] 29%|‚ñà‚ñà‚ñâ       | 73/250 [01:48<04:39,  1.58s/it] 30%|‚ñà‚ñà‚ñâ       | 74/250 [01:49<04:27,  1.52s/it] 30%|‚ñà‚ñà‚ñà       | 75/250 [01:51<04:34,  1.57s/it] 30%|‚ñà‚ñà‚ñà       | 76/250 [01:52<04:20,  1.50s/it] 31%|‚ñà‚ñà‚ñà       | 77/250 [01:54<04:34,  1.58s/it] 31%|‚ñà‚ñà‚ñà       | 78/250 [01:56<04:19,  1.51s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 79/250 [01:57<04:23,  1.54s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 80/250 [01:59<04:11,  1.48s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 81/250 [02:00<04:17,  1.52s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 82/250 [02:02<04:09,  1.49s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 83/250 [02:03<04:17,  1.54s/it] 34%|‚ñà‚ñà‚ñà‚ñé      | 84/250 [02:05<04:03,  1.47s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 85/250 [02:06<04:09,  1.51s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 86/250 [02:08<04:01,  1.47s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 87/250 [02:09<04:11,  1.54s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 88/250 [02:11<04:02,  1.50s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 89/250 [02:12<04:13,  1.57s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 90/250 [02:14<03:58,  1.49s/it] 36%|‚ñà‚ñà‚ñà‚ñã      | 91/250 [02:15<04:03,  1.53s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 92/250 [02:17<03:53,  1.48s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 93/250 [02:18<04:02,  1.55s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 94/250 [02:20<03:55,  1.51s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 95/250 [02:22<04:04,  1.58s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 96/250 [02:23<03:54,  1.52s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 97/250 [02:25<04:00,  1.57s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 98/250 [02:26<03:50,  1.51s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 99/250 [02:28<03:57,  1.57s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 100/250 [02:29<03:43,  1.49s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 101/250 [02:31<03:50,  1.54s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 102/250 [02:32<03:39,  1.49s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 103/250 [02:34<03:46,  1.54s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 104/250 [02:35<03:37,  1.49s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 105/250 [02:37<03:43,  1.54s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 106/250 [02:38<03:34,  1.49s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 107/250 [02:40<03:39,  1.54s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 108/250 [02:41<03:28,  1.47s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 109/250 [02:43<03:32,  1.51s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 110/250 [02:44<03:24,  1.46s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 111/250 [02:46<03:33,  1.53s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 112/250 [02:47<03:24,  1.49s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 113/250 [02:49<03:32,  1.55s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 114/250 [02:50<03:23,  1.49s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 115/250 [02:52<03:31,  1.57s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 116/250 [02:53<03:19,  1.49s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 117/250 [02:55<03:25,  1.54s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 118/250 [02:56<03:16,  1.49s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 119/250 [02:58<03:19,  1.52s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 120/250 [02:59<03:09,  1.45s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 121/250 [03:01<03:15,  1.52s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 122/250 [03:02<03:07,  1.46s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 123/250 [03:04<03:14,  1.53s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 124/250 [03:05<03:06,  1.48s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 125/250 [03:07<03:14,  1.55s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 126/250 [03:08<03:04,  1.49s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 127/250 [03:10<03:07,  1.53s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 128/250 [03:11<02:58,  1.46s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 129/250 [03:13<03:07,  1.55s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 130/250 [03:14<02:56,  1.47s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 131/250 [03:16<03:02,  1.53s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 132/250 [03:17<02:54,  1.48s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 133/250 [03:19<02:57,  1.52s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 134/250 [03:20<02:49,  1.46s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 135/250 [03:22<02:55,  1.53s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 136/250 [03:23<02:48,  1.48s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 137/250 [03:25<02:53,  1.54s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 138/250 [03:26<02:45,  1.47s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 139/250 [03:28<02:50,  1.54s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 140/250 [03:29<02:43,  1.49s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 141/250 [03:31<02:47,  1.54s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 142/250 [03:32<02:40,  1.48s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 143/250 [03:34<02:43,  1.53s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 144/250 [03:35<02:37,  1.48s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 145/250 [03:37<02:39,  1.52s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 146/250 [03:38<02:33,  1.48s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 147/250 [03:40<02:38,  1.54s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 148/250 [03:41<02:31,  1.49s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 149/250 [03:43<02:35,  1.54s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 150/250 [03:44<02:29,  1.50s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 151/250 [03:46<02:35,  1.57s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 152/250 [03:48<02:28,  1.52s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 153/250 [03:49<02:31,  1.56s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 154/250 [03:51<02:24,  1.50s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 155/250 [03:52<02:27,  1.55s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 156/250 [03:54<02:21,  1.50s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 157/250 [03:55<02:24,  1.55s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 158/250 [03:57<02:18,  1.51s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 159/250 [03:58<02:20,  1.55s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 160/250 [04:00<02:13,  1.48s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 161/250 [04:01<02:16,  1.54s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 162/250 [04:03<02:09,  1.47s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 163/250 [04:04<02:15,  1.55s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 164/250 [04:06<02:08,  1.50s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 165/250 [04:07<02:12,  1.55s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 166/250 [04:09<02:04,  1.49s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 167/250 [04:10<02:06,  1.53s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 168/250 [04:12<01:59,  1.46s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 169/250 [04:13<02:01,  1.50s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 170/250 [04:15<01:55,  1.45s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 171/250 [04:17<02:05,  1.58s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 172/250 [04:18<01:58,  1.52s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 173/250 [04:20<02:00,  1.56s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 174/250 [04:21<01:52,  1.48s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 175/250 [04:23<01:54,  1.53s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 176/250 [04:24<01:51,  1.50s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 177/250 [04:26<01:54,  1.57s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 178/250 [04:27<01:47,  1.50s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 179/250 [04:29<01:50,  1.56s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 180/250 [04:30<01:44,  1.50s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 181/250 [04:32<01:46,  1.54s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 182/250 [04:33<01:40,  1.48s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 183/250 [04:35<01:43,  1.54s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 184/250 [04:36<01:38,  1.49s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 185/250 [04:38<01:40,  1.55s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 186/250 [04:39<01:35,  1.49s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 187/250 [04:41<01:37,  1.55s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 188/250 [04:42<01:31,  1.48s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 189/250 [04:44<01:33,  1.54s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 190/250 [04:45<01:29,  1.48s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 191/250 [04:47<01:29,  1.52s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 192/250 [04:48<01:24,  1.46s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 193/250 [04:50<01:26,  1.51s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 194/250 [04:51<01:21,  1.46s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 195/250 [04:53<01:23,  1.52s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 196/250 [04:54<01:18,  1.45s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 197/250 [04:56<01:19,  1.50s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 198/250 [04:57<01:15,  1.45s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 199/250 [04:59<01:16,  1.50s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 200/250 [05:00<01:12,  1.45s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 201/250 [05:02<01:15,  1.53s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 202/250 [05:03<01:10,  1.47s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 203/250 [05:05<01:12,  1.54s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 204/250 [05:06<01:07,  1.47s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 205/250 [05:08<01:10,  1.56s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 206/250 [05:09<01:05,  1.49s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 207/250 [05:11<01:06,  1.54s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 208/250 [05:12<01:01,  1.47s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 209/250 [05:14<01:03,  1.54s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 210/250 [05:15<00:59,  1.48s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 211/250 [05:17<00:59,  1.52s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 212/250 [05:18<00:55,  1.46s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 213/250 [05:20<00:56,  1.52s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 214/250 [05:21<00:53,  1.48s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 215/250 [05:23<00:53,  1.53s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 216/250 [05:24<00:49,  1.47s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 217/250 [05:26<00:50,  1.52s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 218/250 [05:27<00:47,  1.49s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 219/250 [05:29<00:48,  1.57s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 220/250 [05:30<00:45,  1.51s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 221/250 [05:32<00:45,  1.56s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 222/250 [05:33<00:41,  1.50s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 223/250 [05:35<00:41,  1.55s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 224/250 [05:36<00:38,  1.48s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 225/250 [05:38<00:38,  1.53s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 226/250 [05:39<00:35,  1.48s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 227/250 [05:41<00:35,  1.53s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 228/250 [05:42<00:32,  1.46s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 229/250 [05:44<00:32,  1.53s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 230/250 [05:45<00:29,  1.50s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 231/250 [05:47<00:29,  1.56s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 232/250 [05:48<00:27,  1.52s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 233/250 [05:50<00:26,  1.57s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 234/250 [05:51<00:23,  1.49s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 235/250 [05:53<00:23,  1.58s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 236/250 [05:55<00:21,  1.52s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 237/250 [05:56<00:20,  1.57s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 238/250 [05:58<00:18,  1.52s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 239/250 [05:59<00:17,  1.55s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 240/250 [06:01<00:14,  1.48s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 241/250 [06:02<00:13,  1.54s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 242/250 [06:04<00:11,  1.48s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 243/250 [06:05<00:10,  1.53s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 244/250 [06:07<00:08,  1.48s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 245/250 [06:08<00:07,  1.51s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 246/250 [06:10<00:05,  1.46s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 247/250 [06:11<00:04,  1.50s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 248/250 [06:12<00:02,  1.44s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 249/250 [06:14<00:01,  1.52s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [06:16<00:00,  1.47s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [06:16<00:00,  1.50s/it]
2025-05-30 08:37:31,144 - INFO - Initial SFT dev set evaluation results: {'eval_loss': nan, 'eval_model_preparation_time': 0.0065, 'eval_runtime': 379.5618, 'eval_samples_per_second': 2.635, 'eval_steps_per_second': 0.659}
2025-05-30 08:37:31,144 - INFO - Starting SFT training...
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 7,824 | Num Epochs = 4 | Total steps = 980
O^O/ \_/ \    Batch size per device = 32 | Gradient accumulation steps = 1
\        /    Data Parallel GPUs = 1 | Total batch size (32 x 1 x 1) = 32
 "-____-"     Trainable parameters = 159,907,840/6,898,454,528 (2.32% trained)
  0%|          | 0/980 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/sorgin1/users/jbarrutia006/viper/scripts/sft/sftrain.py", line 423, in <module>
    main()
  File "/sorgin1/users/jbarrutia006/viper/scripts/sft/sftrain.py", line 392, in main
    trainer.train()
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 359, in _fast_inner_training_loop
  File "<string>", line 68, in _unsloth_training_step
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/accelerate/accelerator.py", line 2246, in backward
    loss.backward(**kwargs)
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/autograd/function.py", line 307, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/cut_cross_entropy/cce.py", line 94, in backward
    grad_scale = 1 / lse.numel()
                 ~~^~~~~~~~~~~~~
ZeroDivisionError: division by zero
Traceback (most recent call last):
  File "/sorgin1/users/jbarrutia006/viper/scripts/sft/sftrain.py", line 423, in <module>
    main()
  File "/sorgin1/users/jbarrutia006/viper/scripts/sft/sftrain.py", line 392, in main
    trainer.train()
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 359, in _fast_inner_training_loop
  File "<string>", line 68, in _unsloth_training_step
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/accelerate/accelerator.py", line 2246, in backward
    loss.backward(**kwargs)
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/autograd/function.py", line 307, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/cut_cross_entropy/cce.py", line 94, in backward
    grad_scale = 1 / lse.numel()
                 ~~^~~~~~~~~~~~~
ZeroDivisionError: division by zero
srun: error: localhost: task 0: Exited with exit code 1
