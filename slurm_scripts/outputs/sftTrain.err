wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbarrutia006 (jbarrutia006-upv-ehu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /sorgin1/users/jbarrutia006/viper/wandb/run-20250415_001817-xfvz4obl
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run Llama with SFT dataset. First go!
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jbarrutia006-upv-ehu/viperSFT
wandb: üöÄ View run at https://wandb.ai/jbarrutia006-upv-ehu/viperSFT/runs/xfvz4obl
2025-04-15 00:18:20,407 - INFO - Results will be saved to: ./sft_trained_models/04-15_00-18-20
2025-04-15 00:18:20,408 - INFO - Loading model and tokenizer...
Unsloth 2025.3.14 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
2025-04-15 00:19:34,629 - INFO - Loading SFT train and dev datasets...
/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
Unsloth: Tokenizing ["text"] (num_proc=128):   0%|          | 0/8374 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   1%|          | 66/8374 [00:01<03:55, 35.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   2%|‚ñè         | 132/8374 [00:02<01:57, 69.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   2%|‚ñè         | 198/8374 [00:03<02:43, 50.13 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   5%|‚ñç         | 396/8374 [00:04<01:20, 99.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   6%|‚ñå         | 462/8374 [00:05<01:06, 118.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   6%|‚ñã         | 528/8374 [00:05<01:06, 117.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   7%|‚ñã         | 594/8374 [00:06<01:09, 112.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   8%|‚ñä         | 660/8374 [00:07<01:12, 106.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   9%|‚ñä         | 726/8374 [00:07<01:12, 105.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   9%|‚ñâ         | 792/8374 [00:08<01:18, 96.47 examples/s] Unsloth: Tokenizing ["text"] (num_proc=128):  10%|‚ñà         | 858/8374 [00:08<00:59, 125.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  11%|‚ñà         | 924/8374 [00:09<01:10, 105.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  12%|‚ñà‚ñè        | 990/8374 [00:09<01:02, 118.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  13%|‚ñà‚ñé        | 1056/8374 [00:10<01:06, 110.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  13%|‚ñà‚ñé        | 1122/8374 [00:11<01:01, 117.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  14%|‚ñà‚ñç        | 1188/8374 [00:11<01:06, 107.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  15%|‚ñà‚ñç        | 1254/8374 [00:12<01:00, 118.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  16%|‚ñà‚ñå        | 1320/8374 [00:12<01:00, 117.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  17%|‚ñà‚ñã        | 1386/8374 [00:13<00:57, 120.57 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  17%|‚ñà‚ñã        | 1452/8374 [00:14<01:02, 111.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  18%|‚ñà‚ñä        | 1518/8374 [00:14<00:57, 119.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  19%|‚ñà‚ñâ        | 1584/8374 [00:15<01:12, 93.35 examples/s] Unsloth: Tokenizing ["text"] (num_proc=128):  20%|‚ñà‚ñâ        | 1650/8374 [00:16<01:05, 102.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  20%|‚ñà‚ñà        | 1716/8374 [00:16<00:55, 120.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  21%|‚ñà‚ñà‚ñè       | 1782/8374 [00:16<00:54, 120.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  22%|‚ñà‚ñà‚ñè       | 1848/8374 [00:18<01:10, 92.74 examples/s] Unsloth: Tokenizing ["text"] (num_proc=128):  23%|‚ñà‚ñà‚ñé       | 1914/8374 [00:18<00:52, 122.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  24%|‚ñà‚ñà‚ñé       | 1980/8374 [00:18<00:48, 130.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  24%|‚ñà‚ñà‚ñç       | 2046/8374 [00:19<00:56, 111.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  25%|‚ñà‚ñà‚ñå       | 2112/8374 [00:20<00:58, 106.36 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  26%|‚ñà‚ñà‚ñå       | 2178/8374 [00:20<00:50, 122.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  27%|‚ñà‚ñà‚ñã       | 2244/8374 [00:21<00:51, 120.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  28%|‚ñà‚ñà‚ñä       | 2310/8374 [00:22<01:04, 94.51 examples/s] Unsloth: Tokenizing ["text"] (num_proc=128):  28%|‚ñà‚ñà‚ñä       | 2376/8374 [00:22<00:52, 113.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  29%|‚ñà‚ñà‚ñâ       | 2442/8374 [00:22<00:47, 123.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  30%|‚ñà‚ñà‚ñâ       | 2508/8374 [00:23<00:59, 99.02 examples/s] Unsloth: Tokenizing ["text"] (num_proc=128):  31%|‚ñà‚ñà‚ñà       | 2574/8374 [00:23<00:44, 129.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  32%|‚ñà‚ñà‚ñà‚ñè      | 2640/8374 [00:24<00:54, 105.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  32%|‚ñà‚ñà‚ñà‚ñè      | 2706/8374 [00:25<00:45, 124.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  33%|‚ñà‚ñà‚ñà‚ñé      | 2772/8374 [00:25<00:50, 111.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  34%|‚ñà‚ñà‚ñà‚ñç      | 2838/8374 [00:26<00:50, 108.93 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  35%|‚ñà‚ñà‚ñà‚ñç      | 2904/8374 [00:27<00:51, 106.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  35%|‚ñà‚ñà‚ñà‚ñå      | 2970/8374 [00:27<00:42, 127.05 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  36%|‚ñà‚ñà‚ñà‚ñã      | 3036/8374 [00:28<00:45, 116.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  37%|‚ñà‚ñà‚ñà‚ñã      | 3102/8374 [00:28<00:45, 115.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  38%|‚ñà‚ñà‚ñà‚ñä      | 3168/8374 [00:29<00:47, 109.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  39%|‚ñà‚ñà‚ñà‚ñä      | 3234/8374 [00:29<00:42, 120.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  39%|‚ñà‚ñà‚ñà‚ñâ      | 3300/8374 [00:30<00:47, 106.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  40%|‚ñà‚ñà‚ñà‚ñà      | 3366/8374 [00:30<00:40, 124.66 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  41%|‚ñà‚ñà‚ñà‚ñà      | 3432/8374 [00:31<00:38, 126.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 3498/8374 [00:32<00:44, 109.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3564/8374 [00:32<00:39, 120.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3629/8374 [00:33<00:44, 106.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 3694/8374 [00:33<00:37, 124.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 3759/8374 [00:34<00:38, 121.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 3824/8374 [00:34<00:40, 112.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 3889/8374 [00:35<00:39, 114.62 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 3954/8374 [00:36<00:39, 111.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 4019/8374 [00:37<00:50, 86.40 examples/s] Unsloth: Tokenizing ["text"] (num_proc=128):  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 4149/8374 [00:38<00:42, 99.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4214/8374 [00:38<00:35, 117.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4279/8374 [00:39<00:35, 114.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 4344/8374 [00:40<00:39, 101.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 4409/8374 [00:40<00:31, 125.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 4474/8374 [00:40<00:32, 120.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 4539/8374 [00:41<00:32, 117.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 4604/8374 [00:41<00:30, 123.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 4669/8374 [00:42<00:29, 125.51 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4734/8374 [00:43<00:33, 108.08 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4799/8374 [00:43<00:32, 108.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 4864/8374 [00:44<00:34, 103.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 4929/8374 [00:45<00:33, 103.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 4994/8374 [00:45<00:30, 110.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 5059/8374 [00:46<00:28, 118.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 5124/8374 [00:46<00:31, 104.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 5189/8374 [00:47<00:37, 84.53 examples/s] Unsloth: Tokenizing ["text"] (num_proc=128):  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5254/8374 [00:48<00:34, 90.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5319/8374 [00:49<00:33, 91.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 5384/8374 [00:49<00:30, 98.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 5449/8374 [00:50<00:30, 96.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 5514/8374 [00:51<00:29, 98.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 5579/8374 [00:51<00:25, 109.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 5644/8374 [00:52<00:28, 97.25 examples/s] Unsloth: Tokenizing ["text"] (num_proc=128):  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 5709/8374 [00:52<00:26, 100.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 5774/8374 [00:53<00:24, 104.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 5839/8374 [00:53<00:21, 116.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 5904/8374 [00:54<00:23, 105.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 5969/8374 [00:55<00:23, 102.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 6034/8374 [00:55<00:20, 113.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 6099/8374 [00:56<00:22, 102.44 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 6164/8374 [00:57<00:23, 95.09 examples/s] Unsloth: Tokenizing ["text"] (num_proc=128):  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 6229/8374 [00:58<00:22, 93.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6294/8374 [00:58<00:18, 111.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6359/8374 [00:59<00:20, 98.41 examples/s] Unsloth: Tokenizing ["text"] (num_proc=128):  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 6424/8374 [00:59<00:18, 103.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 6489/8374 [01:00<00:16, 111.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 6554/8374 [01:00<00:17, 105.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 6619/8374 [01:01<00:18, 96.36 examples/s] Unsloth: Tokenizing ["text"] (num_proc=128):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 6684/8374 [01:02<00:16, 100.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 6749/8374 [01:02<00:14, 109.98 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 6814/8374 [01:03<00:15, 101.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 6879/8374 [01:04<00:13, 109.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 6944/8374 [01:04<00:15, 94.59 examples/s] Unsloth: Tokenizing ["text"] (num_proc=128):  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 7009/8374 [01:05<00:12, 106.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 7074/8374 [01:06<00:13, 93.26 examples/s] Unsloth: Tokenizing ["text"] (num_proc=128):  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 7139/8374 [01:06<00:11, 110.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 7204/8374 [01:07<00:10, 115.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 7269/8374 [01:07<00:10, 102.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7334/8374 [01:08<00:10, 100.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7399/8374 [01:09<00:08, 115.48 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 7464/8374 [01:09<00:07, 120.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 7529/8374 [01:10<00:07, 118.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 7594/8374 [01:10<00:07, 102.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 7659/8374 [01:11<00:06, 110.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 7724/8374 [01:11<00:05, 125.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 7789/8374 [01:12<00:05, 115.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 7854/8374 [01:12<00:04, 121.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 7919/8374 [01:13<00:04, 112.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 7984/8374 [01:14<00:03, 117.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 8049/8374 [01:14<00:02, 121.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 8114/8374 [01:15<00:02, 108.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 8179/8374 [01:15<00:01, 126.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 8244/8374 [01:16<00:01, 106.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 8309/8374 [01:16<00:00, 111.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8374/8374 [01:17<00:00, 117.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8374/8374 [01:17<00:00, 107.80 examples/s]
Unsloth: Tokenizing ["text"] (num_proc=128):   0%|          | 0/500 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   1%|          | 4/500 [00:00<01:41,  4.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   2%|‚ñè         | 8/500 [00:01<00:57,  8.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   3%|‚ñé         | 16/500 [00:01<00:30, 15.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   5%|‚ñç         | 24/500 [00:01<00:20, 23.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   6%|‚ñå         | 28/500 [00:01<00:19, 24.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   6%|‚ñã         | 32/500 [00:01<00:18, 25.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   7%|‚ñã         | 36/500 [00:01<00:17, 26.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   8%|‚ñä         | 40/500 [00:02<00:17, 26.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   9%|‚ñâ         | 44/500 [00:02<00:17, 26.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  10%|‚ñâ         | 48/500 [00:02<00:16, 28.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  10%|‚ñà         | 52/500 [00:02<00:14, 30.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  11%|‚ñà         | 56/500 [00:02<00:14, 30.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  12%|‚ñà‚ñè        | 60/500 [00:02<00:14, 30.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  13%|‚ñà‚ñé        | 64/500 [00:03<00:23, 18.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  14%|‚ñà‚ñç        | 72/500 [00:03<00:18, 23.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  16%|‚ñà‚ñå        | 80/500 [00:03<00:17, 23.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  18%|‚ñà‚ñä        | 88/500 [00:03<00:13, 31.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  20%|‚ñà‚ñà        | 100/500 [00:03<00:10, 37.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  22%|‚ñà‚ñà‚ñè       | 108/500 [00:04<00:10, 36.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  23%|‚ñà‚ñà‚ñé       | 116/500 [00:04<00:09, 42.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  25%|‚ñà‚ñà‚ñç       | 124/500 [00:04<00:09, 40.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  26%|‚ñà‚ñà‚ñã       | 132/500 [00:04<00:10, 34.22 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  27%|‚ñà‚ñà‚ñã       | 136/500 [00:04<00:10, 34.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  28%|‚ñà‚ñà‚ñä       | 140/500 [00:05<00:10, 34.74 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  30%|‚ñà‚ñà‚ñâ       | 148/500 [00:05<00:08, 41.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  31%|‚ñà‚ñà‚ñà       | 156/500 [00:05<00:08, 39.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  33%|‚ñà‚ñà‚ñà‚ñé      | 164/500 [00:05<00:08, 38.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  34%|‚ñà‚ñà‚ñà‚ñç      | 172/500 [00:05<00:09, 35.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  35%|‚ñà‚ñà‚ñà‚ñå      | 176/500 [00:06<00:09, 35.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  36%|‚ñà‚ñà‚ñà‚ñå      | 180/500 [00:06<00:10, 31.02 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  37%|‚ñà‚ñà‚ñà‚ñã      | 184/500 [00:06<00:10, 29.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  38%|‚ñà‚ñà‚ñà‚ñä      | 188/500 [00:06<00:10, 29.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  38%|‚ñà‚ñà‚ñà‚ñä      | 192/500 [00:06<00:10, 28.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  39%|‚ñà‚ñà‚ñà‚ñâ      | 196/500 [00:06<00:10, 28.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  40%|‚ñà‚ñà‚ñà‚ñà      | 200/500 [00:06<00:09, 30.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  41%|‚ñà‚ñà‚ñà‚ñà      | 204/500 [00:07<00:12, 23.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 212/500 [00:07<00:10, 28.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 216/500 [00:07<00:10, 27.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 220/500 [00:07<00:09, 28.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 224/500 [00:07<00:10, 27.39 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 228/500 [00:07<00:09, 28.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 232/500 [00:08<00:08, 29.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 236/500 [00:08<00:08, 30.85 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 240/500 [00:08<00:10, 24.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 248/500 [00:08<00:07, 34.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 252/500 [00:08<00:08, 28.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 260/500 [00:08<00:06, 37.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 268/500 [00:09<00:07, 30.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 276/500 [00:09<00:05, 37.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 284/500 [00:09<00:06, 31.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 292/500 [00:09<00:05, 38.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 300/500 [00:10<00:06, 32.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 308/500 [00:10<00:05, 38.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 316/500 [00:10<00:04, 37.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 324/500 [00:10<00:04, 35.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 332/500 [00:10<00:04, 34.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 336/500 [00:11<00:04, 34.56 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 340/500 [00:11<00:04, 33.58 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 344/500 [00:11<00:04, 33.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 348/500 [00:11<00:04, 32.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 352/500 [00:11<00:04, 33.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 356/500 [00:11<00:04, 31.89 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 360/500 [00:11<00:04, 31.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 364/500 [00:11<00:04, 32.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 368/500 [00:12<00:03, 33.60 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 372/500 [00:12<00:05, 24.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 376/500 [00:12<00:04, 26.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 384/500 [00:12<00:03, 36.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 388/500 [00:12<00:03, 35.69 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 392/500 [00:12<00:03, 32.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 396/500 [00:13<00:03, 33.06 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 400/500 [00:13<00:03, 32.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 408/500 [00:13<00:02, 32.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 412/500 [00:13<00:02, 33.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 416/500 [00:13<00:02, 31.09 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 420/500 [00:13<00:02, 32.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 424/500 [00:13<00:02, 31.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 428/500 [00:14<00:02, 31.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 432/500 [00:14<00:02, 24.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 436/500 [00:14<00:02, 26.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 440/500 [00:14<00:02, 28.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 448/500 [00:14<00:01, 37.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 452/500 [00:14<00:01, 33.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 456/500 [00:14<00:01, 30.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 464/500 [00:15<00:01, 30.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 470/500 [00:15<00:01, 27.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 473/500 [00:15<00:01, 26.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 476/500 [00:15<00:00, 25.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 482/500 [00:15<00:00, 26.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 485/500 [00:16<00:00, 21.55 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 488/500 [00:16<00:00, 21.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 491/500 [00:16<00:00, 21.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 494/500 [00:16<00:00, 19.33 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:16<00:00, 23.41 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [00:17<00:00, 29.35 examples/s]
2025-04-15 00:21:32,679 - WARNING - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-04-15 00:21:32,707 - INFO - Performing an initial evaluation on the dev_sft dataset...
  0%|          | 0/63 [00:00<?, ?it/s]  3%|‚ñé         | 2/63 [00:00<00:19,  3.05it/s]  5%|‚ñç         | 3/63 [00:01<00:28,  2.12it/s]  6%|‚ñã         | 4/63 [00:01<00:31,  1.86it/s]  8%|‚ñä         | 5/63 [00:02<00:33,  1.73it/s] 10%|‚ñâ         | 6/63 [00:03<00:34,  1.66it/s] 11%|‚ñà         | 7/63 [00:03<00:34,  1.62it/s] 13%|‚ñà‚ñé        | 8/63 [00:04<00:34,  1.59it/s] 14%|‚ñà‚ñç        | 9/63 [00:05<00:34,  1.58it/s] 16%|‚ñà‚ñå        | 10/63 [00:05<00:33,  1.57it/s] 17%|‚ñà‚ñã        | 11/63 [00:06<00:33,  1.56it/s] 19%|‚ñà‚ñâ        | 12/63 [00:07<00:32,  1.55it/s] 21%|‚ñà‚ñà        | 13/63 [00:07<00:32,  1.55it/s] 22%|‚ñà‚ñà‚ñè       | 14/63 [00:08<00:31,  1.54it/s] 24%|‚ñà‚ñà‚ñç       | 15/63 [00:09<00:31,  1.54it/s] 25%|‚ñà‚ñà‚ñå       | 16/63 [00:09<00:30,  1.53it/s] 27%|‚ñà‚ñà‚ñã       | 17/63 [00:10<00:30,  1.52it/s] 29%|‚ñà‚ñà‚ñä       | 18/63 [00:11<00:29,  1.52it/s] 30%|‚ñà‚ñà‚ñà       | 19/63 [00:11<00:28,  1.52it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 20/63 [00:12<00:28,  1.53it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 21/63 [00:13<00:27,  1.53it/s] 35%|‚ñà‚ñà‚ñà‚ñç      | 22/63 [00:13<00:26,  1.54it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 23/63 [00:14<00:25,  1.54it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 24/63 [00:15<00:25,  1.53it/s] 40%|‚ñà‚ñà‚ñà‚ñâ      | 25/63 [00:15<00:24,  1.54it/s] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 26/63 [00:16<00:24,  1.54it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 27/63 [00:16<00:23,  1.54it/s] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 28/63 [00:17<00:22,  1.54it/s] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 29/63 [00:18<00:22,  1.54it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 30/63 [00:18<00:21,  1.53it/s] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 31/63 [00:19<00:21,  1.50it/s] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 32/63 [00:20<00:20,  1.50it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 33/63 [00:20<00:20,  1.50it/s] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 34/63 [00:21<00:19,  1.51it/s] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 35/63 [00:22<00:18,  1.52it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 36/63 [00:22<00:17,  1.53it/s] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 37/63 [00:23<00:17,  1.53it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 38/63 [00:24<00:16,  1.53it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 39/63 [00:24<00:15,  1.52it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 40/63 [00:25<00:15,  1.52it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 41/63 [00:26<00:14,  1.53it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 42/63 [00:26<00:13,  1.53it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 43/63 [00:27<00:13,  1.53it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 44/63 [00:28<00:12,  1.53it/s] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 45/63 [00:28<00:11,  1.53it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 46/63 [00:29<00:11,  1.53it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 47/63 [00:30<00:10,  1.53it/s] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 48/63 [00:30<00:09,  1.51it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 49/63 [00:31<00:09,  1.51it/s] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 50/63 [00:32<00:08,  1.52it/s] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 51/63 [00:32<00:07,  1.53it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 52/63 [00:33<00:07,  1.53it/s] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 53/63 [00:34<00:06,  1.53it/s] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 54/63 [00:34<00:05,  1.53it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 55/63 [00:35<00:05,  1.52it/s] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 56/63 [00:36<00:04,  1.52it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 57/63 [00:36<00:03,  1.53it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 58/63 [00:37<00:03,  1.53it/s] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 59/63 [00:37<00:02,  1.53it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 60/63 [00:38<00:01,  1.53it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 61/63 [00:39<00:01,  1.53it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 62/63 [00:39<00:00,  1.53it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:40<00:00,  1.74it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63/63 [00:40<00:00,  1.56it/s]
2025-04-15 00:22:21,604 - INFO - Initial SFT dev set evaluation results: {'eval_loss': nan, 'eval_model_preparation_time': 0.0091, 'eval_runtime': 48.8587, 'eval_samples_per_second': 10.234, 'eval_steps_per_second': 1.289}
2025-04-15 00:22:21,604 - INFO - Starting SFT training...
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 8,374 | Num Epochs = 6 | Total steps = 3,144
O^O/ \_/ \    Batch size per device = 16 | Gradient accumulation steps = 1
\        /    Data Parallel GPUs = 1 | Total batch size (16 x 1 x 1) = 16
 "-____-"     Trainable parameters = 167,772,160/8,000,000,000 (2.10% trained)
  0%|          | 0/3144 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/sorgin1/users/jbarrutia006/viper/scripts/sft/sftrain.py", line 316, in <module>
    main()
  File "/sorgin1/users/jbarrutia006/viper/scripts/sft/sftrain.py", line 285, in main
    trainer.train()
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 359, in _fast_inner_training_loop
  File "<string>", line 68, in _unsloth_training_step
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/accelerate/accelerator.py", line 2246, in backward
    loss.backward(**kwargs)
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/autograd/function.py", line 307, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/cut_cross_entropy/cce.py", line 94, in backward
    grad_scale = 1 / lse.numel()
                 ~~^~~~~~~~~~~~~
ZeroDivisionError: division by zero
Traceback (most recent call last):
  File "/sorgin1/users/jbarrutia006/viper/scripts/sft/sftrain.py", line 316, in <module>
    main()
  File "/sorgin1/users/jbarrutia006/viper/scripts/sft/sftrain.py", line 285, in main
    trainer.train()
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 359, in _fast_inner_training_loop
  File "<string>", line 68, in _unsloth_training_step
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/accelerate/accelerator.py", line 2246, in backward
    loss.backward(**kwargs)
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/autograd/function.py", line 307, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/cut_cross_entropy/cce.py", line 94, in backward
    grad_scale = 1 / lse.numel()
                 ~~^~~~~~~~~~~~~
ZeroDivisionError: division by zero
srun: error: localhost: task 0: Exited with exit code 1
