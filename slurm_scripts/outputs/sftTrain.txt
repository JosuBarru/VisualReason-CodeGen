ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
INFO 05-30 08:27:13 __init__.py:183] Automatically detected platform cuda.
==((====))==  Unsloth 2025.3.14: Fast Llama patching. Transformers: 4.48.2. vLLM: 0.7.1.
   \\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.325 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.5.1+cu121. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
codellama/CodeLlama-7b-Instruct-hf does not have a padding token! Will use pad_token = <unk>.
trainable params: 159,907,840 || all params: 6,898,454,528 || trainable%: 2.3180
Unsloth: We found double BOS tokens - we shall remove one automatically.
Unsloth: We found double BOS tokens - we shall remove one automatically.
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mcodellama[0m at: [34mhttps://wandb.ai/jbarrutia006-upv-ehu/viperSFT/runs/sxrqhtm4[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250530_082716-sxrqhtm4/logs[0m
