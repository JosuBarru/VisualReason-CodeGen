ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
INFO 04-15 19:16:24 __init__.py:183] Automatically detected platform cuda.
==((====))==  Unsloth 2025.3.14: Fast Llama patching. Transformers: 4.48.2. vLLM: 0.7.1.
   \\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.325 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.5.1+cu121. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: We found double BOS tokens - we shall remove one automatically.
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 9.6298, 'grad_norm': 7.8486762046813965, 'learning_rate': 1.2658227848101267e-05, 'epoch': 0.15}
{'loss': 4.1831, 'grad_norm': 0.6057196259498596, 'learning_rate': 2.5316455696202533e-05, 'epoch': 0.31}
{'eval_loss': 2.412199020385742, 'eval_runtime': 448.6204, 'eval_samples_per_second': 1.115, 'eval_steps_per_second': 0.279, 'epoch': 0.38}
{'loss': 3.7513, 'grad_norm': 0.6862164735794067, 'learning_rate': 3.79746835443038e-05, 'epoch': 0.46}
{'loss': 3.636, 'grad_norm': 1.0692960023880005, 'learning_rate': 4.9999753185758066e-05, 'epoch': 0.61}
{'loss': 3.7721, 'grad_norm': 0.38529497385025024, 'learning_rate': 4.989123369922547e-05, 'epoch': 0.76}
{'eval_loss': 2.3699896335601807, 'eval_runtime': 444.4262, 'eval_samples_per_second': 1.125, 'eval_steps_per_second': 0.281, 'epoch': 0.76}
{'loss': 3.6272, 'grad_norm': 0.5564582347869873, 'learning_rate': 4.958625089578343e-05, 'epoch': 0.92}
{'loss': 3.6374, 'grad_norm': 0.13310669362545013, 'learning_rate': 4.908721196560535e-05, 'epoch': 1.07}
{'eval_loss': 2.3623549938201904, 'eval_runtime': 446.4647, 'eval_samples_per_second': 1.12, 'eval_steps_per_second': 0.28, 'epoch': 1.15}
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mLlama with SFT dataset. First go![0m at: [34mhttps://wandb.ai/jbarrutia006-upv-ehu/viperSFT/runs/frpevqno[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250415_190808-frpevqno/logs[0m
