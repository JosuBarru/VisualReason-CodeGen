ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
INFO 04-15 23:13:02 __init__.py:183] Automatically detected platform cuda.
==((====))==  Unsloth 2025.3.14: Fast Llama patching. Transformers: 4.48.2. vLLM: 0.7.1.
   \\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.325 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.5.1+cu121. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: We found double BOS tokens - we shall remove one automatically.
Unsloth: We found double BOS tokens - we shall remove one automatically.
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 12.9778, 'grad_norm': 45.5, 'learning_rate': 3.7037037037037037e-06, 'epoch': 0.04}
{'loss': 10.7088, 'grad_norm': 21.0, 'learning_rate': 7.4074074074074075e-06, 'epoch': 0.08}
{'loss': 8.5605, 'grad_norm': 12.9375, 'learning_rate': 9.995979423530893e-06, 'epoch': 0.11}
{'loss': 6.3356, 'grad_norm': 7.3125, 'learning_rate': 9.92468224365778e-06, 'epoch': 0.15}
{'loss': 5.3261, 'grad_norm': 4.0625, 'learning_rate': 9.765503992478704e-06, 'epoch': 0.19}
{'loss': 4.793, 'grad_norm': 2.0, 'learning_rate': 9.521285209656964e-06, 'epoch': 0.23}
{'loss': 4.1711, 'grad_norm': 1.484375, 'learning_rate': 9.196383985157657e-06, 'epoch': 0.27}
{'loss': 4.2145, 'grad_norm': 0.859375, 'learning_rate': 8.79659818902903e-06, 'epoch': 0.31}
{'loss': 4.083, 'grad_norm': 0.609375, 'learning_rate': 8.3290620082741e-06, 'epoch': 0.34}
{'loss': 3.7683, 'grad_norm': 0.7421875, 'learning_rate': 7.802118637114575e-06, 'epoch': 0.38}
{'eval_loss': 2.9060628414154053, 'eval_model_preparation_time': 0.0066, 'eval_runtime': 442.4412, 'eval_samples_per_second': 1.13, 'eval_steps_per_second': 0.283, 'epoch': 0.38}
{'loss': 3.8501, 'grad_norm': 0.53125, 'learning_rate': 7.225171392492316e-06, 'epoch': 0.42}
{'loss': 3.7023, 'grad_norm': 0.62109375, 'learning_rate': 6.608515911655744e-06, 'epoch': 0.46}
{'loss': 3.742, 'grad_norm': 0.30859375, 'learning_rate': 5.963156426269228e-06, 'epoch': 0.5}
{'loss': 3.528, 'grad_norm': 0.2734375, 'learning_rate': 5.300609391638336e-06, 'epoch': 0.53}
{'loss': 3.6898, 'grad_norm': 0.37109375, 'learning_rate': 4.6326979752920905e-06, 'epoch': 0.57}
{'loss': 3.6369, 'grad_norm': 0.5546875, 'learning_rate': 3.971341072278302e-06, 'epoch': 0.61}
{'loss': 4.0346, 'grad_norm': 0.3359375, 'learning_rate': 3.328340612199091e-06, 'epoch': 0.65}
{'loss': 3.5679, 'grad_norm': 0.9375, 'learning_rate': 2.715170953497532e-06, 'epoch': 0.69}
{'loss': 3.8449, 'grad_norm': 0.2890625, 'learning_rate': 2.1427741232594185e-06, 'epoch': 0.73}
{'loss': 3.6366, 'grad_norm': 0.2890625, 'learning_rate': 1.6213645564806751e-06, 'epoch': 0.76}
{'eval_loss': 2.7718887329101562, 'eval_model_preparation_time': 0.0066, 'eval_runtime': 441.3968, 'eval_samples_per_second': 1.133, 'eval_steps_per_second': 0.283, 'epoch': 0.76}
{'loss': 3.6228, 'grad_norm': 0.3671875, 'learning_rate': 1.1602468192328936e-06, 'epoch': 0.8}
{'loss': 3.6293, 'grad_norm': 0.2333984375, 'learning_rate': 7.676495684613433e-07, 'epoch': 0.84}
{'loss': 3.5647, 'grad_norm': 0.251953125, 'learning_rate': 4.505787114068433e-07, 'epoch': 0.88}
{'loss': 3.7111, 'grad_norm': 0.27734375, 'learning_rate': 2.1469238502507926e-07, 'epoch': 0.92}
{'loss': 3.513, 'grad_norm': 0.2734375, 'learning_rate': 6.419998639858538e-08, 'epoch': 0.95}
{'loss': 3.6318, 'grad_norm': 0.263671875, 'learning_rate': 1.7870559460814173e-09, 'epoch': 0.99}
{'train_runtime': 7255.2114, 'train_samples_per_second': 1.154, 'train_steps_per_second': 0.036, 'train_loss': 4.754740019790999, 'epoch': 1.0}
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mLlama with SFT dataset. 1 epoch[0m at: [34mhttps://wandb.ai/jbarrutia006-upv-ehu/viperSFT/runs/s57vum63[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250415_231306-s57vum63/logs[0m
