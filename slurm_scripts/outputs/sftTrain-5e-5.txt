ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
INFO 04-16 08:46:55 __init__.py:183] Automatically detected platform cuda.
==((====))==  Unsloth 2025.3.14: Fast Llama patching. Transformers: 4.48.2. vLLM: 0.7.1.
   \\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.325 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.5.1+cu121. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: We found double BOS tokens - we shall remove one automatically.
Unsloth: We found double BOS tokens - we shall remove one automatically.
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 12.1277, 'grad_norm': 21.0, 'learning_rate': 9.523809523809523e-06, 'epoch': 0.08}
{'loss': 6.7862, 'grad_norm': 3.953125, 'learning_rate': 1.9047619047619046e-05, 'epoch': 0.15}
{'eval_loss': 3.4888415336608887, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 441.8329, 'eval_samples_per_second': 1.132, 'eval_steps_per_second': 0.283, 'epoch': 0.19}
{'loss': 4.2438, 'grad_norm': 0.734375, 'learning_rate': 2.857142857142857e-05, 'epoch': 0.23}
{'loss': 3.6854, 'grad_norm': 0.5625, 'learning_rate': 3.809523809523809e-05, 'epoch': 0.31}
{'loss': 3.7669, 'grad_norm': 0.91796875, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.38}
{'eval_loss': 2.4005751609802246, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 437.6209, 'eval_samples_per_second': 1.143, 'eval_steps_per_second': 0.286, 'epoch': 0.38}
{'loss': 3.721, 'grad_norm': 0.6484375, 'learning_rate': 4.9968791096430083e-05, 'epoch': 0.46}
{'loss': 3.6121, 'grad_norm': 0.470703125, 'learning_rate': 4.983024196138457e-05, 'epoch': 0.53}
{'eval_loss': 2.3822762966156006, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 441.75, 'eval_samples_per_second': 1.132, 'eval_steps_per_second': 0.283, 'epoch': 0.57}
{'loss': 3.6508, 'grad_norm': 0.259765625, 'learning_rate': 4.9581499161993266e-05, 'epoch': 0.61}
{'loss': 3.7967, 'grad_norm': 0.2099609375, 'learning_rate': 4.922366658925276e-05, 'epoch': 0.69}
{'loss': 3.7403, 'grad_norm': 0.2333984375, 'learning_rate': 4.875833226161547e-05, 'epoch': 0.76}
{'eval_loss': 2.371990919113159, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 437.6115, 'eval_samples_per_second': 1.143, 'eval_steps_per_second': 0.286, 'epoch': 0.76}
{'loss': 3.6182, 'grad_norm': 0.91015625, 'learning_rate': 4.8187561277552374e-05, 'epoch': 0.84}
{'loss': 3.6316, 'grad_norm': 0.291015625, 'learning_rate': 4.751388665089127e-05, 'epoch': 0.92}
{'eval_loss': 2.3616561889648438, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 441.4123, 'eval_samples_per_second': 1.133, 'eval_steps_per_second': 0.283, 'epoch': 0.95}
{'loss': 3.563, 'grad_norm': 0.2890625, 'learning_rate': 4.674029806960234e-05, 'epoch': 0.99}
{'loss': 3.7072, 'grad_norm': 0.2451171875, 'learning_rate': 4.5870228627918056e-05, 'epoch': 1.07}
{'loss': 3.6701, 'grad_norm': 0.6875, 'learning_rate': 4.490753959066876e-05, 'epoch': 1.15}
{'eval_loss': 2.358372211456299, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 437.3144, 'eval_samples_per_second': 1.143, 'eval_steps_per_second': 0.286, 'epoch': 1.15}
{'loss': 3.6053, 'grad_norm': 0.140625, 'learning_rate': 4.385650325744803e-05, 'epoch': 1.22}
{'loss': 3.6579, 'grad_norm': 0.2021484375, 'learning_rate': 4.272178400265456e-05, 'epoch': 1.3}
{'eval_loss': 2.364767074584961, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 441.988, 'eval_samples_per_second': 1.131, 'eval_steps_per_second': 0.283, 'epoch': 1.34}
{'loss': 3.6973, 'grad_norm': 0.64453125, 'learning_rate': 4.150841757555265e-05, 'epoch': 1.37}
{'loss': 3.6389, 'grad_norm': 0.1923828125, 'learning_rate': 4.022178875221515e-05, 'epoch': 1.45}
{'loss': 3.5554, 'grad_norm': 0.109375, 'learning_rate': 3.886760743852685e-05, 'epoch': 1.53}
{'eval_loss': 2.349792957305908, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 436.4595, 'eval_samples_per_second': 1.146, 'eval_steps_per_second': 0.286, 'epoch': 1.53}
{'loss': 3.7332, 'grad_norm': 0.333984375, 'learning_rate': 3.745188333030048e-05, 'epoch': 1.6}
{'loss': 3.7374, 'grad_norm': 0.1748046875, 'learning_rate': 3.5980899242960495e-05, 'epoch': 1.68}
{'eval_loss': 2.3566222190856934, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 441.0493, 'eval_samples_per_second': 1.134, 'eval_steps_per_second': 0.283, 'epoch': 1.72}
{'loss': 3.6983, 'grad_norm': 0.11083984375, 'learning_rate': 3.446118322915456e-05, 'epoch': 1.76}
{'loss': 3.6911, 'grad_norm': 0.11181640625, 'learning_rate': 3.2899479608031354e-05, 'epoch': 1.83}
{'loss': 3.7036, 'grad_norm': 0.279296875, 'learning_rate': 3.130271903475328e-05, 'epoch': 1.91}
{'eval_loss': 2.3551998138427734, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 437.4071, 'eval_samples_per_second': 1.143, 'eval_steps_per_second': 0.286, 'epoch': 1.91}
{'loss': 3.7158, 'grad_norm': 0.265625, 'learning_rate': 2.9677987743072163e-05, 'epoch': 1.98}
{'loss': 3.6335, 'grad_norm': 0.1455078125, 'learning_rate': 2.8032496097465556e-05, 'epoch': 2.06}
{'eval_loss': 2.351071357727051, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 441.8985, 'eval_samples_per_second': 1.131, 'eval_steps_per_second': 0.283, 'epoch': 2.1}
{'loss': 3.6631, 'grad_norm': 0.2314453125, 'learning_rate': 2.6373546594395708e-05, 'epoch': 2.14}
{'loss': 3.5832, 'grad_norm': 0.1572265625, 'learning_rate': 2.4708501454697693e-05, 'epoch': 2.21}
{'loss': 3.4896, 'grad_norm': 0.2265625, 'learning_rate': 2.3044749950917856e-05, 'epoch': 2.29}
{'eval_loss': 2.3457350730895996, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 437.1152, 'eval_samples_per_second': 1.144, 'eval_steps_per_second': 0.286, 'epoch': 2.29}
{'loss': 3.7017, 'grad_norm': 0.2021484375, 'learning_rate': 2.1389675614600013e-05, 'epoch': 2.37}
{'loss': 3.6479, 'grad_norm': 0.1318359375, 'learning_rate': 1.9750623469049492e-05, 'epoch': 2.44}
{'eval_loss': 2.3443381786346436, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 443.3047, 'eval_samples_per_second': 1.128, 'eval_steps_per_second': 0.282, 'epoch': 2.48}
{'loss': 3.5367, 'grad_norm': 0.365234375, 'learning_rate': 1.813486743299222e-05, 'epoch': 2.52}
{'loss': 3.6376, 'grad_norm': 0.150390625, 'learning_rate': 1.6549578039787436e-05, 'epoch': 2.6}
{'loss': 3.6766, 'grad_norm': 0.318359375, 'learning_rate': 1.5001790615452383e-05, 'epoch': 2.67}
{'eval_loss': 2.347846031188965, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 437.1791, 'eval_samples_per_second': 1.144, 'eval_steps_per_second': 0.286, 'epoch': 2.67}
{'loss': 3.5723, 'grad_norm': 0.279296875, 'learning_rate': 1.3498374056721197e-05, 'epoch': 2.75}
{'loss': 3.675, 'grad_norm': 0.2216796875, 'learning_rate': 1.2046000347697058e-05, 'epoch': 2.82}
{'eval_loss': 2.3477327823638916, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 441.539, 'eval_samples_per_second': 1.132, 'eval_steps_per_second': 0.283, 'epoch': 2.86}
{'loss': 3.6348, 'grad_norm': 0.458984375, 'learning_rate': 1.0651114950379312e-05, 'epoch': 2.9}
{'loss': 3.7394, 'grad_norm': 0.380859375, 'learning_rate': 9.319908200468769e-06, 'epoch': 2.98}
{'loss': 3.6346, 'grad_norm': 0.1396484375, 'learning_rate': 8.058287835393132e-06, 'epoch': 3.05}
{'eval_loss': 2.346548557281494, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 437.1833, 'eval_samples_per_second': 1.144, 'eval_steps_per_second': 0.286, 'epoch': 3.05}
{'loss': 3.7201, 'grad_norm': 0.357421875, 'learning_rate': 6.871852776470089e-06, 'epoch': 3.13}
{'loss': 3.6384, 'grad_norm': 0.333984375, 'learning_rate': 5.765868281559575e-06, 'epoch': 3.21}
{'eval_loss': 2.346928596496582, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 441.6074, 'eval_samples_per_second': 1.132, 'eval_steps_per_second': 0.283, 'epoch': 3.24}
{'loss': 3.6964, 'grad_norm': 0.37890625, 'learning_rate': 4.745242578474798e-06, 'epoch': 3.28}
{'loss': 3.7144, 'grad_norm': 0.177734375, 'learning_rate': 3.8145050828502755e-06, 'epoch': 3.36}
{'loss': 3.6663, 'grad_norm': 0.138671875, 'learning_rate': 2.977786297133234e-06, 'epoch': 3.44}
{'eval_loss': 2.34678316116333, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 437.0347, 'eval_samples_per_second': 1.144, 'eval_steps_per_second': 0.286, 'epoch': 3.44}
{'loss': 3.7481, 'grad_norm': 0.09912109375, 'learning_rate': 2.238799479904413e-06, 'epoch': 3.51}
{'loss': 3.6391, 'grad_norm': 0.11767578125, 'learning_rate': 1.6008241668775848e-06, 'epoch': 3.59}
{'eval_loss': 2.3468382358551025, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 441.7885, 'eval_samples_per_second': 1.132, 'eval_steps_per_second': 0.283, 'epoch': 3.63}
{'loss': 3.5577, 'grad_norm': 0.1865234375, 'learning_rate': 1.0666916167095836e-06, 'epoch': 3.66}
{'loss': 3.664, 'grad_norm': 0.208984375, 'learning_rate': 6.387722462107171e-07, 'epoch': 3.74}
{'loss': 3.6712, 'grad_norm': 0.2216796875, 'learning_rate': 3.189651107165459e-07, 'epoch': 3.82}
{'eval_loss': 2.346937894821167, 'eval_model_preparation_time': 0.0064, 'eval_runtime': 436.8697, 'eval_samples_per_second': 1.145, 'eval_steps_per_second': 0.286, 'epoch': 3.82}
{'loss': 3.7847, 'grad_norm': 0.1826171875, 'learning_rate': 1.0868947630585936e-07, 'epoch': 3.89}
{'loss': 3.7358, 'grad_norm': 0.13671875, 'learning_rate': 8.878521266422746e-09, 'epoch': 3.97}
{'train_runtime': 34171.1161, 'train_samples_per_second': 0.98, 'train_steps_per_second': 0.031, 'train_loss': 3.8982732623588037, 'epoch': 4.0}
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mLlama with SFT dataset. 3 epochs. Lr 5e-5[0m at: [34mhttps://wandb.ai/jbarrutia006-upv-ehu/viperSFT/runs/vrz505tz[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250416_084658-vrz505tz/logs[0m
