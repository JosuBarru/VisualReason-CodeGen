INFO:root:{'multiprocessing': False, 'path_pretrained_models': './pretrained_models', 'execute_code': False, 'dataset': {'dataset_name': 'GQA', 'data_path': './data/gqa', 'split': 'train', 'max_samples': 8, 'batch_size': 8, 'start_sample': 0, 'testing': False}, 'load_models': {'maskrcnn': False, 'clip': False, 'glip': False, 'owlvit': False, 'tcl': False, 'gpt3_qa': False, 'gpt3_general': False, 'depth': False, 'blip': False, 'saliency': False, 'xvlm': False, 'codex': False, 'codellama': True, 'codellama_Q': False, 'llm_query': False, 'llm_guess': False, 'gpt3_list': False, 'qa': False, 'guess': False, 'gpt3_guess': False}, 'detect_thresholds': {'glip': 0.5, 'maskrcnn': 0.8, 'owlvit': 0.1}, 'ratio_box_area_to_image_area': 0.0, 'crop_larger_margin': True, 'verify_property': {'model': 'xvlm', 'thresh_clip': 0.6, 'thresh_tcl': 0.25, 'thresh_xvlm': 0.6}, 'best_match_model': 'xvlm', 'gpt3': {'n_votes': 1, 'qa_prompt': './prompts/gpt3/gpt3_qa.txt', 'guess_prompt': './prompts/gpt3/gpt3_process_guess.txt', 'temperature': 0.0, 'model': 'text-davinci-003'}, 'codex': {'temperature': 0.0, 'best_of': 1, 'max_tokens': 512, 'prompt': './prompts/benchmarks/gqa.prompt', 'model': 'codellama', 'extra_context': None, 'model_name': 'codellama/CodeLlama-7b-Instruct-hf'}, 'save': False, 'save_new_results': True, 'save_codex': True, 'results_dir': './results/gqa/codex_results/', 'use_cache': True, 'clear_cache': True, 'log_every': 20, 'wandb': False, 'blip_half_precision': False, 'blip_v2_model_type': 'blip2-flan-t5-xl', 'glip_model_type': 'large', 'use_fixed_code': False, 'fixed_code_file': './prompts/fixed_code/blip2.prompt', 'cognition': {'is_setted': False}, 'use_cached_codex': False, 'use_cache_codex': False}
INFO:__main__:Starting main
Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  50% Completed | 1/2 [01:37<01:37, 97.01s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [02:07<00:00, 58.13s/it]
Loading safetensors checkpoint shards: 100% Completed | 2/2 [02:07<00:00, 63.96s/it]

Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:00<00:20,  1.65it/s]Capturing CUDA graph shapes:   6%|▌         | 2/35 [00:01<00:19,  1.70it/s]Capturing CUDA graph shapes:   9%|▊         | 3/35 [00:01<00:18,  1.74it/s]Capturing CUDA graph shapes:  11%|█▏        | 4/35 [00:02<00:17,  1.73it/s]Capturing CUDA graph shapes:  14%|█▍        | 5/35 [00:02<00:16,  1.76it/s]Capturing CUDA graph shapes:  17%|█▋        | 6/35 [00:03<00:16,  1.77it/s]Capturing CUDA graph shapes:  20%|██        | 7/35 [00:03<00:15,  1.79it/s]Capturing CUDA graph shapes:  23%|██▎       | 8/35 [00:04<00:14,  1.81it/s]Capturing CUDA graph shapes:  26%|██▌       | 9/35 [00:05<00:14,  1.82it/s]Capturing CUDA graph shapes:  29%|██▊       | 10/35 [00:05<00:13,  1.82it/s]Capturing CUDA graph shapes:  31%|███▏      | 11/35 [00:06<00:13,  1.83it/s]Capturing CUDA graph shapes:  34%|███▍      | 12/35 [00:06<00:12,  1.83it/s]Capturing CUDA graph shapes:  37%|███▋      | 13/35 [00:07<00:12,  1.80it/s]Capturing CUDA graph shapes:  40%|████      | 14/35 [00:07<00:11,  1.82it/s]Capturing CUDA graph shapes:  43%|████▎     | 15/35 [00:08<00:10,  1.83it/s]Capturing CUDA graph shapes:  46%|████▌     | 16/35 [00:08<00:10,  1.84it/s]Capturing CUDA graph shapes:  49%|████▊     | 17/35 [00:09<00:09,  1.84it/s]Capturing CUDA graph shapes:  51%|█████▏    | 18/35 [00:09<00:09,  1.86it/s]Capturing CUDA graph shapes:  54%|█████▍    | 19/35 [00:10<00:08,  1.87it/s]Capturing CUDA graph shapes:  57%|█████▋    | 20/35 [00:11<00:08,  1.86it/s]Capturing CUDA graph shapes:  60%|██████    | 21/35 [00:11<00:07,  1.88it/s]Capturing CUDA graph shapes:  63%|██████▎   | 22/35 [00:12<00:06,  1.89it/s]Capturing CUDA graph shapes:  66%|██████▌   | 23/35 [00:12<00:06,  1.90it/s]Capturing CUDA graph shapes:  69%|██████▊   | 24/35 [00:13<00:05,  1.86it/s]Capturing CUDA graph shapes:  71%|███████▏  | 25/35 [00:14<00:06,  1.55it/s]Capturing CUDA graph shapes:  74%|███████▍  | 26/35 [00:14<00:05,  1.63it/s]Capturing CUDA graph shapes:  77%|███████▋  | 27/35 [00:15<00:04,  1.71it/s]Capturing CUDA graph shapes:  80%|████████  | 28/35 [00:15<00:03,  1.77it/s]Capturing CUDA graph shapes:  83%|████████▎ | 29/35 [00:16<00:03,  1.81it/s]Capturing CUDA graph shapes:  86%|████████▌ | 30/35 [00:16<00:02,  1.85it/s]Capturing CUDA graph shapes:  89%|████████▊ | 31/35 [00:17<00:02,  1.88it/s]Capturing CUDA graph shapes:  91%|█████████▏| 32/35 [00:17<00:01,  1.90it/s]Capturing CUDA graph shapes:  94%|█████████▍| 33/35 [00:18<00:01,  1.91it/s]Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:18<00:00,  1.92it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:19<00:00,  1.89it/s]Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:19<00:00,  1.82it/s]
INFO:__main__:Models successfully loaded
WARNING:joblib:[Memory(location=cache/joblib)]: Flushing completely the cache
INFO:__main__:Dataset loaded
  0%|                                                                         | 0/1 [00:00<?, ?it/s]
Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][A
Processed prompts:  12%|█▎        | 1/8 [00:02<00:18,  2.58s/it, est. speed input: 1339.36 toks/s, output: 14.74 toks/s][A
Processed prompts:  25%|██▌       | 2/8 [00:10<00:34,  5.82s/it, est. speed input: 647.61 toks/s, output: 40.77 toks/s] [A
Processed prompts:  38%|███▊      | 3/8 [00:12<00:19,  4.00s/it, est. speed input: 830.03 toks/s, output: 74.51 toks/s][A
Processed prompts:  50%|█████     | 4/8 [00:12<00:10,  2.53s/it, est. speed input: 1083.06 toks/s, output: 113.00 toks/s][AProcessed prompts: 100%|██████████| 8/8 [00:12<00:00,  1.60s/it, est. speed input: 2166.31 toks/s, output: 273.36 toks/s]
/sorgin1/users/jbarrutia006/viper/src/main_project_slurm.py:275: UserWarning: Not executing code! This is only generating the code. We set the flag 'execute_code' to False by default, because executing code generated by a language model can be dangerous. Set the flag 'execute_code' to True if you want to execute it.
  warnings.warn("Not executing code! This is only generating the code. We set the flag "
100%|#################################################################| 1/1 [00:12<00:00, 12.94s/it]100%|#################################################################| 1/1 [00:12<00:00, 12.94s/it]
INFO:__main__:Saving results to codellama___03-09_11-47.csv
[rank0]:[W309 11:47:16.166953051 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
