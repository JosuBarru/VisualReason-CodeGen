ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!
INFO 04-15 19:26:55 __init__.py:183] Automatically detected platform cuda.
==((====))==  Unsloth 2025.3.14: Fast Llama patching. Transformers: 4.48.2. vLLM: 0.7.1.
   \\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.325 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.5.1+cu121. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: We found double BOS tokens - we shall remove one automatically.
Unsloth: We found double BOS tokens - we shall remove one automatically.
Unsloth: Will smartly offload gradients to save VRAM!
{'loss': 0.3185, 'grad_norm': 0.0022362577728927135, 'learning_rate': 2.5316455696202533e-05, 'epoch': 0.31}
{'loss': 0.0012, 'grad_norm': 0.0, 'learning_rate': 4.9999753185758066e-05, 'epoch': 0.61}
{'eval_loss': 0.0009596640593372285, 'eval_runtime': 86.8229, 'eval_samples_per_second': 5.759, 'eval_steps_per_second': 1.44, 'epoch': 0.76}
{'loss': 0.0011, 'grad_norm': 0.0, 'learning_rate': 4.958625089578343e-05, 'epoch': 0.92}
{'loss': 0.001, 'grad_norm': 0.0, 'learning_rate': 4.839805575899791e-05, 'epoch': 1.22}
{'loss': 0.001, 'grad_norm': 0.0, 'learning_rate': 4.647260684152773e-05, 'epoch': 1.53}
{'eval_loss': 0.0004729825886897743, 'eval_runtime': 81.1883, 'eval_samples_per_second': 6.159, 'eval_steps_per_second': 1.54, 'epoch': 1.53}
{'loss': 0.001, 'grad_norm': 0.0, 'learning_rate': 4.387057347883143e-05, 'epoch': 1.83}
{'loss': 0.0009, 'grad_norm': 0.0, 'learning_rate': 4.0673943634091705e-05, 'epoch': 2.14}
{'eval_loss': 0.0003999227483291179, 'eval_runtime': 83.4422, 'eval_samples_per_second': 5.992, 'eval_steps_per_second': 1.498, 'epoch': 2.29}
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mLlama with SFT dataset. Prueba menor max_seq_len[0m at: [34mhttps://wandb.ai/jbarrutia006-upv-ehu/viperSFT/runs/t3q3ujro[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250415_192658-t3q3ujro/logs[0m
