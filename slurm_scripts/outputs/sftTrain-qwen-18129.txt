ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.
ü¶• Unsloth Zoo will now patch everything to make training faster!
INFO 06-10 15:37:35 __init__.py:183] Automatically detected platform cuda.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: jbarrutia006 (jbarrutia006-upv-ehu) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /sorgin1/users/jbarrutia006/viper/wandb/run-20250610_153739-2nr04e1h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run qwen
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jbarrutia006-upv-ehu/viperSFT
wandb: üöÄ View run at https://wandb.ai/jbarrutia006-upv-ehu/viperSFT/runs/2nr04e1h
2025-06-10 15:37:41,340 - INFO - Results will be saved to: ./sft_trained_models/06-10_15-37-41
2025-06-10 15:37:41,340 - INFO - Loading model and tokenizer...
==((====))==  Unsloth 2025.3.14: Fast Qwen2 patching. Transformers: 4.48.2. vLLM: 0.7.1.
   \\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.151 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.5.1+cu121. CUDA: 8.0. CUDA Toolkit: 12.1. Triton: 3.1.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [01:15<03:47, 75.82s/it]Downloading shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [13:01<14:52, 446.42s/it]Downloading shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [14:15<04:36, 276.25s/it]Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [14:45<00:00, 178.89s/it]Downloading shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [14:45<00:00, 221.26s/it]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:42<02:06, 42.22s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [01:25<01:25, 42.62s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [02:02<00:40, 40.36s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [02:12<00:00, 28.25s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [02:12<00:00, 33.12s/it]
Unsloth 2025.3.14 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.
2025-06-10 15:54:54,019 - INFO - Loading SFT train and dev datasets...
trainable params: 161,480,704 || all params: 7,777,097,216 || trainable%: 2.0764
Formateando train SFT:   0%|          | 0/7824 [00:00<?, ? examples/s]Formateando train SFT:  13%|‚ñà‚ñé        | 1000/7824 [00:00<00:01, 6471.79 examples/s]Formateando train SFT:  26%|‚ñà‚ñà‚ñå       | 2000/7824 [00:00<00:00, 7412.32 examples/s]Formateando train SFT:  38%|‚ñà‚ñà‚ñà‚ñä      | 3000/7824 [00:00<00:00, 7809.80 examples/s]Formateando train SFT:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4000/7824 [00:00<00:00, 8026.89 examples/s]Formateando train SFT:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 5000/7824 [00:00<00:00, 8141.66 examples/s]Formateando train SFT:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 6000/7824 [00:00<00:00, 8239.99 examples/s]Formateando train SFT:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 7000/7824 [00:00<00:00, 8256.96 examples/s]Formateando train SFT: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7824/7824 [00:02<00:00, 3866.21 examples/s]
Formateando dev SFT:   0%|          | 0/1000 [00:00<?, ? examples/s]Formateando dev SFT: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 7849.83 examples/s]Formateando dev SFT: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 3693.38 examples/s]
/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
Unsloth: Tokenizing ["text"] (num_proc=128):   0%|          | 0/7824 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   1%|          | 62/7824 [00:00<01:52, 68.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   2%|‚ñè         | 186/7824 [00:01<00:35, 214.84 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   5%|‚ñç         | 372/7824 [00:01<00:20, 365.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   6%|‚ñã         | 496/7824 [00:01<00:16, 452.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   8%|‚ñä         | 620/7824 [00:01<00:13, 523.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  10%|‚ñà         | 806/7824 [00:01<00:10, 679.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  12%|‚ñà‚ñè        | 930/7824 [00:02<00:10, 681.21 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  13%|‚ñà‚ñé        | 1053/7824 [00:02<00:09, 723.49 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  15%|‚ñà‚ñå        | 1175/7824 [00:02<00:08, 762.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  17%|‚ñà‚ñã        | 1297/7824 [00:02<00:08, 778.67 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  18%|‚ñà‚ñä        | 1419/7824 [00:02<00:09, 678.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  20%|‚ñà‚ñâ        | 1541/7824 [00:02<00:10, 616.70 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  22%|‚ñà‚ñà‚ñè       | 1724/7824 [00:03<00:09, 659.31 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  24%|‚ñà‚ñà‚ñé       | 1846/7824 [00:03<00:08, 739.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  25%|‚ñà‚ñà‚ñå       | 1968/7824 [00:03<00:07, 792.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  27%|‚ñà‚ñà‚ñã       | 2090/7824 [00:03<00:07, 767.80 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  28%|‚ñà‚ñà‚ñä       | 2212/7824 [00:03<00:07, 779.03 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  30%|‚ñà‚ñà‚ñâ       | 2334/7824 [00:03<00:06, 810.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  31%|‚ñà‚ñà‚ñà‚ñè      | 2456/7824 [00:04<00:06, 768.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  33%|‚ñà‚ñà‚ñà‚ñé      | 2578/7824 [00:04<00:07, 738.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  35%|‚ñà‚ñà‚ñà‚ñç      | 2700/7824 [00:04<00:07, 724.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  37%|‚ñà‚ñà‚ñà‚ñã      | 2883/7824 [00:04<00:06, 717.43 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  39%|‚ñà‚ñà‚ñà‚ñâ      | 3066/7824 [00:04<00:06, 719.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 3249/7824 [00:05<00:05, 821.16 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 3371/7824 [00:05<00:05, 822.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 3493/7824 [00:05<00:05, 728.99 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 3615/7824 [00:05<00:06, 665.00 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 3737/7824 [00:05<00:06, 619.27 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 3859/7824 [00:06<00:06, 645.61 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 4042/7824 [00:06<00:05, 754.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 4164/7824 [00:06<00:04, 748.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 4286/7824 [00:06<00:05, 676.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 4408/7824 [00:06<00:04, 702.40 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 4591/7824 [00:06<00:03, 817.07 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 4713/7824 [00:07<00:03, 792.71 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 4835/7824 [00:07<00:04, 702.24 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 5018/7824 [00:07<00:03, 755.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 5140/7824 [00:07<00:03, 777.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 5262/7824 [00:07<00:03, 781.10 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 5384/7824 [00:08<00:03, 783.96 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 5567/7824 [00:08<00:02, 873.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 5689/7824 [00:08<00:02, 853.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 5811/7824 [00:08<00:02, 744.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 6055/7824 [00:08<00:02, 773.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 6177/7824 [00:09<00:02, 764.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 6299/7824 [00:09<00:01, 776.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 6421/7824 [00:09<00:01, 766.46 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 6543/7824 [00:09<00:01, 848.81 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 6665/7824 [00:09<00:01, 803.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 6787/7824 [00:09<00:01, 801.86 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 6909/7824 [00:09<00:01, 810.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 7031/7824 [00:10<00:01, 783.14 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 7153/7824 [00:10<00:00, 775.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 7275/7824 [00:10<00:00, 804.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 7397/7824 [00:10<00:00, 830.18 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 7519/7824 [00:10<00:00, 827.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 7641/7824 [00:10<00:00, 785.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 7763/7824 [00:10<00:00, 807.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7824/7824 [00:11<00:00, 701.44 examples/s]
Unsloth: Tokenizing ["text"] (num_proc=128):   0%|          | 0/1000 [00:00<?, ? examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   1%|          | 8/1000 [00:00<01:06, 14.92 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   2%|‚ñè         | 24/1000 [00:00<00:23, 41.97 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   4%|‚ñç         | 40/1000 [00:00<00:15, 61.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   6%|‚ñå         | 56/1000 [00:00<00:13, 72.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   7%|‚ñã         | 72/1000 [00:01<00:10, 88.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):   9%|‚ñâ         | 88/1000 [00:01<00:09, 96.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  10%|‚ñà         | 104/1000 [00:01<00:10, 89.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  12%|‚ñà‚ñè        | 120/1000 [00:01<00:09, 88.32 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  14%|‚ñà‚ñç        | 144/1000 [00:01<00:08, 95.87 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  16%|‚ñà‚ñå        | 160/1000 [00:02<00:09, 91.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  18%|‚ñà‚ñä        | 184/1000 [00:02<00:07, 110.26 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  20%|‚ñà‚ñà        | 200/1000 [00:02<00:07, 110.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  22%|‚ñà‚ñà‚ñè       | 216/1000 [00:02<00:07, 109.19 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  23%|‚ñà‚ñà‚ñé       | 232/1000 [00:02<00:07, 105.76 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  25%|‚ñà‚ñà‚ñç       | 248/1000 [00:02<00:07, 106.04 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  26%|‚ñà‚ñà‚ñã       | 264/1000 [00:03<00:07, 96.33 examples/s] Unsloth: Tokenizing ["text"] (num_proc=128):  28%|‚ñà‚ñà‚ñä       | 280/1000 [00:03<00:07, 100.91 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  30%|‚ñà‚ñà‚ñâ       | 296/1000 [00:03<00:06, 102.59 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  31%|‚ñà‚ñà‚ñà       | 312/1000 [00:03<00:07, 94.07 examples/s] Unsloth: Tokenizing ["text"] (num_proc=128):  33%|‚ñà‚ñà‚ñà‚ñé      | 328/1000 [00:03<00:06, 99.45 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  35%|‚ñà‚ñà‚ñà‚ñå      | 352/1000 [00:03<00:05, 113.38 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  37%|‚ñà‚ñà‚ñà‚ñã      | 368/1000 [00:03<00:05, 113.78 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  38%|‚ñà‚ñà‚ñà‚ñä      | 384/1000 [00:04<00:05, 114.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  40%|‚ñà‚ñà‚ñà‚ñà      | 400/1000 [00:04<00:05, 116.37 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 416/1000 [00:04<00:05, 112.75 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 432/1000 [00:04<00:05, 110.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 448/1000 [00:04<00:04, 111.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 464/1000 [00:04<00:04, 109.53 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 480/1000 [00:04<00:04, 111.54 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 496/1000 [00:05<00:04, 112.20 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 512/1000 [00:05<00:04, 108.79 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 528/1000 [00:05<00:04, 109.50 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 544/1000 [00:05<00:04, 110.63 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 560/1000 [00:05<00:03, 111.94 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 576/1000 [00:05<00:03, 109.17 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 592/1000 [00:05<00:03, 110.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 608/1000 [00:06<00:03, 113.29 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 624/1000 [00:06<00:03, 112.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 640/1000 [00:06<00:03, 110.28 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 656/1000 [00:06<00:03, 111.82 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 672/1000 [00:06<00:02, 114.72 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 688/1000 [00:06<00:02, 115.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 704/1000 [00:06<00:02, 116.25 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 720/1000 [00:07<00:02, 112.77 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 736/1000 [00:07<00:02, 111.88 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 752/1000 [00:07<00:02, 109.42 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 768/1000 [00:07<00:02, 111.12 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 784/1000 [00:07<00:01, 113.23 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 800/1000 [00:07<00:01, 113.83 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 816/1000 [00:07<00:01, 114.65 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 832/1000 [00:08<00:01, 113.52 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 846/1000 [00:08<00:01, 104.35 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 860/1000 [00:08<00:01, 101.01 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 874/1000 [00:08<00:01, 101.34 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 888/1000 [00:08<00:01, 103.95 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 902/1000 [00:08<00:00, 102.68 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 916/1000 [00:09<00:00, 87.18 examples/s] Unsloth: Tokenizing ["text"] (num_proc=128):  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 944/1000 [00:09<00:00, 101.47 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 965/1000 [00:09<00:00, 108.11 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 979/1000 [00:09<00:00, 104.64 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128):  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 993/1000 [00:09<00:00, 107.90 examples/s]Unsloth: Tokenizing ["text"] (num_proc=128): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:09<00:00, 101.96 examples/s]
2025-06-10 15:55:25,676 - WARNING - Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2025-06-10 15:55:25,682 - INFO - Performing an initial evaluation on the dev_sft dataset...
  0%|          | 0/250 [00:00<?, ?it/s]  1%|          | 2/250 [00:02<05:11,  1.26s/it]  1%|          | 3/250 [00:05<07:32,  1.83s/it]  2%|‚ñè         | 4/250 [00:07<08:36,  2.10s/it]  2%|‚ñè         | 5/250 [00:10<09:06,  2.23s/it]  2%|‚ñè         | 6/250 [00:12<09:24,  2.31s/it]  3%|‚ñé         | 7/250 [00:15<09:40,  2.39s/it]  3%|‚ñé         | 8/250 [00:17<09:59,  2.48s/it]  4%|‚ñé         | 9/250 [00:20<09:56,  2.48s/it]  4%|‚ñç         | 10/250 [00:22<10:01,  2.50s/it]  4%|‚ñç         | 11/250 [00:25<10:07,  2.54s/it]  5%|‚ñç         | 12/250 [00:28<10:02,  2.53s/it]  5%|‚ñå         | 13/250 [00:30<10:04,  2.55s/it]  6%|‚ñå         | 14/250 [00:32<09:44,  2.48s/it]  6%|‚ñå         | 15/250 [00:35<09:42,  2.48s/it]  6%|‚ñã         | 16/250 [00:37<09:33,  2.45s/it]  7%|‚ñã         | 17/250 [00:40<09:26,  2.43s/it]  7%|‚ñã         | 18/250 [00:42<09:43,  2.51s/it]  8%|‚ñä         | 19/250 [00:45<09:44,  2.53s/it]  8%|‚ñä         | 20/250 [00:47<09:38,  2.52s/it]  8%|‚ñä         | 21/250 [00:50<09:40,  2.53s/it]  9%|‚ñâ         | 22/250 [00:53<09:38,  2.54s/it]  9%|‚ñâ         | 23/250 [00:55<09:50,  2.60s/it] 10%|‚ñâ         | 24/250 [00:58<09:42,  2.58s/it] 10%|‚ñà         | 25/250 [01:00<09:40,  2.58s/it] 10%|‚ñà         | 26/250 [01:03<09:25,  2.53s/it] 11%|‚ñà         | 27/250 [01:05<09:28,  2.55s/it] 11%|‚ñà         | 28/250 [01:08<09:22,  2.53s/it] 12%|‚ñà‚ñè        | 29/250 [01:10<09:17,  2.52s/it] 12%|‚ñà‚ñè        | 30/250 [01:13<09:15,  2.53s/it] 12%|‚ñà‚ñè        | 31/250 [01:16<09:15,  2.54s/it] 13%|‚ñà‚ñé        | 32/250 [01:18<09:12,  2.53s/it] 13%|‚ñà‚ñé        | 33/250 [01:21<09:14,  2.55s/it] 14%|‚ñà‚ñé        | 34/250 [01:23<08:54,  2.47s/it] 14%|‚ñà‚ñç        | 35/250 [01:26<08:57,  2.50s/it] 14%|‚ñà‚ñç        | 36/250 [01:28<08:56,  2.51s/it] 15%|‚ñà‚ñç        | 37/250 [01:31<08:59,  2.53s/it] 15%|‚ñà‚ñå        | 38/250 [01:33<08:56,  2.53s/it] 16%|‚ñà‚ñå        | 39/250 [01:36<08:51,  2.52s/it] 16%|‚ñà‚ñå        | 40/250 [01:38<08:48,  2.52s/it] 16%|‚ñà‚ñã        | 41/250 [01:41<08:50,  2.54s/it] 17%|‚ñà‚ñã        | 42/250 [01:43<08:43,  2.52s/it] 17%|‚ñà‚ñã        | 43/250 [01:46<08:45,  2.54s/it] 18%|‚ñà‚ñä        | 44/250 [01:48<08:40,  2.53s/it] 18%|‚ñà‚ñä        | 45/250 [01:51<08:33,  2.51s/it] 18%|‚ñà‚ñä        | 46/250 [01:53<08:27,  2.49s/it] 19%|‚ñà‚ñâ        | 47/250 [01:56<08:24,  2.48s/it] 19%|‚ñà‚ñâ        | 48/250 [01:58<08:24,  2.50s/it] 20%|‚ñà‚ñâ        | 49/250 [02:01<08:25,  2.52s/it] 20%|‚ñà‚ñà        | 50/250 [02:03<08:24,  2.52s/it] 20%|‚ñà‚ñà        | 51/250 [02:06<08:29,  2.56s/it] 21%|‚ñà‚ñà        | 52/250 [02:08<08:17,  2.51s/it] 21%|‚ñà‚ñà        | 53/250 [02:11<08:23,  2.55s/it] 22%|‚ñà‚ñà‚ñè       | 54/250 [02:14<08:19,  2.55s/it] 22%|‚ñà‚ñà‚ñè       | 55/250 [02:16<08:13,  2.53s/it] 22%|‚ñà‚ñà‚ñè       | 56/250 [02:18<08:06,  2.51s/it] 23%|‚ñà‚ñà‚ñé       | 57/250 [02:21<08:13,  2.56s/it] 23%|‚ñà‚ñà‚ñé       | 58/250 [02:24<08:01,  2.51s/it] 24%|‚ñà‚ñà‚ñé       | 59/250 [02:26<08:05,  2.54s/it] 24%|‚ñà‚ñà‚ñç       | 60/250 [02:29<08:00,  2.53s/it] 24%|‚ñà‚ñà‚ñç       | 61/250 [02:31<08:04,  2.56s/it] 25%|‚ñà‚ñà‚ñç       | 62/250 [02:34<07:53,  2.52s/it] 25%|‚ñà‚ñà‚ñå       | 63/250 [02:36<07:58,  2.56s/it] 26%|‚ñà‚ñà‚ñå       | 64/250 [02:39<07:56,  2.56s/it] 26%|‚ñà‚ñà‚ñå       | 65/250 [02:42<07:54,  2.57s/it] 26%|‚ñà‚ñà‚ñã       | 66/250 [02:44<07:42,  2.52s/it] 27%|‚ñà‚ñà‚ñã       | 67/250 [02:47<07:45,  2.54s/it] 27%|‚ñà‚ñà‚ñã       | 68/250 [02:49<07:43,  2.55s/it] 28%|‚ñà‚ñà‚ñä       | 69/250 [02:52<07:44,  2.56s/it] 28%|‚ñà‚ñà‚ñä       | 70/250 [02:54<07:33,  2.52s/it] 28%|‚ñà‚ñà‚ñä       | 71/250 [02:57<07:36,  2.55s/it] 29%|‚ñà‚ñà‚ñâ       | 72/250 [02:59<07:33,  2.55s/it] 29%|‚ñà‚ñà‚ñâ       | 73/250 [03:02<07:45,  2.63s/it] 30%|‚ñà‚ñà‚ñâ       | 74/250 [03:05<07:39,  2.61s/it] 30%|‚ñà‚ñà‚ñà       | 75/250 [03:07<07:37,  2.61s/it] 30%|‚ñà‚ñà‚ñà       | 76/250 [03:10<07:31,  2.59s/it] 31%|‚ñà‚ñà‚ñà       | 77/250 [03:12<07:32,  2.61s/it] 31%|‚ñà‚ñà‚ñà       | 78/250 [03:15<07:24,  2.58s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 79/250 [03:17<07:16,  2.55s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 80/250 [03:20<07:11,  2.54s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 81/250 [03:23<07:10,  2.55s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 82/250 [03:25<07:09,  2.56s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 83/250 [03:28<07:05,  2.55s/it] 34%|‚ñà‚ñà‚ñà‚ñé      | 84/250 [03:30<06:54,  2.50s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 85/250 [03:33<06:51,  2.49s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 86/250 [03:35<06:51,  2.51s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 87/250 [03:38<06:49,  2.51s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 88/250 [03:40<06:44,  2.50s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 89/250 [03:43<06:46,  2.52s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 90/250 [03:45<06:38,  2.49s/it] 36%|‚ñà‚ñà‚ñà‚ñã      | 91/250 [03:48<06:40,  2.52s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 92/250 [03:50<06:38,  2.52s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 93/250 [03:53<06:33,  2.51s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 94/250 [03:55<06:34,  2.53s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 95/250 [03:58<06:45,  2.62s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 96/250 [04:01<06:46,  2.64s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 97/250 [04:03<06:43,  2.64s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 98/250 [04:06<06:37,  2.61s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 99/250 [04:08<06:32,  2.60s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 100/250 [04:11<06:21,  2.54s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 101/250 [04:14<06:22,  2.57s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 102/250 [04:16<06:18,  2.56s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 103/250 [04:19<06:16,  2.56s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 104/250 [04:21<06:13,  2.56s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 105/250 [04:24<06:14,  2.59s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 106/250 [04:26<06:11,  2.58s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 107/250 [04:29<06:11,  2.60s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 108/250 [04:31<05:56,  2.51s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 109/250 [04:34<05:52,  2.50s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 110/250 [04:36<05:51,  2.51s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 111/250 [04:39<05:52,  2.54s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 112/250 [04:41<05:50,  2.54s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 113/250 [04:44<05:51,  2.57s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 114/250 [04:47<05:48,  2.56s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 115/250 [04:49<05:47,  2.57s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 116/250 [04:52<05:37,  2.52s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 117/250 [04:54<05:38,  2.55s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 118/250 [04:57<05:35,  2.54s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 119/250 [04:59<05:29,  2.52s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 120/250 [05:02<05:22,  2.48s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 121/250 [05:04<05:21,  2.49s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 122/250 [05:07<05:19,  2.49s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 123/250 [05:09<05:20,  2.52s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 124/250 [05:12<05:14,  2.49s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 125/250 [05:14<05:17,  2.54s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 126/250 [05:17<05:13,  2.53s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 127/250 [05:19<05:09,  2.52s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 128/250 [05:22<05:03,  2.49s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 129/250 [05:24<05:02,  2.50s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 130/250 [05:27<04:56,  2.47s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 131/250 [05:29<05:00,  2.52s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 132/250 [05:32<04:58,  2.53s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 133/250 [05:34<04:54,  2.51s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 134/250 [05:37<04:47,  2.48s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 135/250 [05:39<04:54,  2.56s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 136/250 [05:42<04:47,  2.52s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 137/250 [05:45<04:48,  2.55s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 138/250 [05:47<04:43,  2.54s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 139/250 [05:50<04:42,  2.55s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 140/250 [05:52<04:36,  2.51s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 141/250 [05:55<04:36,  2.54s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 142/250 [05:57<04:33,  2.53s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 143/250 [06:00<04:32,  2.55s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 144/250 [06:02<04:30,  2.55s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 145/250 [06:05<04:25,  2.53s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 146/250 [06:07<04:24,  2.54s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 147/250 [06:10<04:24,  2.57s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 148/250 [06:13<04:21,  2.57s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 149/250 [06:15<04:16,  2.54s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 150/250 [06:18<04:12,  2.52s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 151/250 [06:20<04:16,  2.59s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 152/250 [06:23<04:09,  2.55s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 153/250 [06:25<04:08,  2.56s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 154/250 [06:28<04:01,  2.52s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 155/250 [06:30<03:58,  2.51s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 156/250 [06:33<03:57,  2.53s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 157/250 [06:35<03:58,  2.56s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 158/250 [06:38<03:53,  2.53s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 159/250 [06:40<03:52,  2.55s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 160/250 [06:43<03:45,  2.51s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 161/250 [06:45<03:46,  2.54s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 162/250 [06:48<03:36,  2.46s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 163/250 [06:51<03:44,  2.58s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 164/250 [06:53<03:40,  2.57s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 165/250 [06:56<03:39,  2.59s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 166/250 [06:58<03:32,  2.53s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 167/250 [07:01<03:27,  2.50s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 168/250 [07:03<03:22,  2.48s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 169/250 [07:05<03:19,  2.47s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 170/250 [07:08<03:17,  2.47s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 171/250 [07:11<03:21,  2.56s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 172/250 [07:13<03:16,  2.52s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 173/250 [07:16<03:16,  2.55s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 174/250 [07:18<03:10,  2.50s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 175/250 [07:21<03:09,  2.53s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 176/250 [07:23<03:08,  2.54s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 177/250 [07:26<03:11,  2.62s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 178/250 [07:29<03:06,  2.58s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 179/250 [07:31<03:05,  2.61s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 180/250 [07:34<02:59,  2.56s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 181/250 [07:36<02:56,  2.56s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 182/250 [07:39<02:52,  2.54s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 183/250 [07:41<02:52,  2.57s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 184/250 [07:44<02:49,  2.57s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 185/250 [07:47<02:48,  2.58s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 186/250 [07:49<02:44,  2.57s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 187/250 [07:52<02:41,  2.56s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 188/250 [07:54<02:33,  2.48s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 189/250 [07:57<02:33,  2.52s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 190/250 [07:59<02:29,  2.49s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 191/250 [08:01<02:24,  2.46s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 192/250 [08:04<02:23,  2.47s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 193/250 [08:06<02:20,  2.47s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 194/250 [08:09<02:17,  2.45s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 195/250 [08:11<02:14,  2.45s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 196/250 [08:14<02:12,  2.46s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 197/250 [08:16<02:11,  2.49s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 198/250 [08:19<02:09,  2.49s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 199/250 [08:21<02:08,  2.51s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 200/250 [08:24<02:05,  2.51s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 201/250 [08:27<02:07,  2.60s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 202/250 [08:29<02:03,  2.57s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 203/250 [08:32<02:01,  2.59s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 204/250 [08:34<01:56,  2.53s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 205/250 [08:37<01:54,  2.54s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 206/250 [08:39<01:51,  2.53s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 207/250 [08:42<01:48,  2.53s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 208/250 [08:44<01:45,  2.52s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 209/250 [08:47<01:44,  2.56s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 210/250 [08:49<01:41,  2.54s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 211/250 [08:52<01:38,  2.51s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 212/250 [08:54<01:33,  2.45s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 213/250 [08:57<01:32,  2.50s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 214/250 [08:59<01:29,  2.47s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 215/250 [09:02<01:27,  2.50s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 216/250 [09:04<01:25,  2.51s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 217/250 [09:07<01:23,  2.53s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 218/250 [09:10<01:22,  2.58s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 219/250 [09:12<01:20,  2.60s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 220/250 [09:15<01:17,  2.59s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 221/250 [09:17<01:14,  2.56s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 222/250 [09:20<01:10,  2.52s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 223/250 [09:22<01:06,  2.47s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 224/250 [09:24<01:03,  2.45s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 225/250 [09:27<01:02,  2.49s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 226/250 [09:30<01:00,  2.51s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 227/250 [09:32<00:57,  2.50s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 228/250 [09:34<00:54,  2.47s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 229/250 [09:37<00:52,  2.52s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 230/250 [09:40<00:50,  2.54s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 231/250 [09:42<00:48,  2.57s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 232/250 [09:45<00:47,  2.61s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 233/250 [09:48<00:44,  2.62s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 234/250 [09:50<00:40,  2.55s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 235/250 [09:53<00:38,  2.57s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 236/250 [09:55<00:35,  2.57s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 237/250 [09:58<00:33,  2.58s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 238/250 [10:00<00:30,  2.58s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 239/250 [10:03<00:28,  2.58s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 240/250 [10:05<00:25,  2.52s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 241/250 [10:08<00:22,  2.55s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 242/250 [10:10<00:20,  2.52s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 243/250 [10:13<00:17,  2.54s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 244/250 [10:16<00:15,  2.53s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 245/250 [10:18<00:12,  2.52s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 246/250 [10:20<00:09,  2.48s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 247/250 [10:23<00:07,  2.45s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 248/250 [10:25<00:04,  2.40s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 249/250 [10:28<00:02,  2.46s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [10:30<00:00,  2.48s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 250/250 [10:30<00:00,  2.52s/it]
2025-06-10 16:06:00,655 - INFO - Initial SFT dev set evaluation results: {'eval_loss': nan, 'eval_model_preparation_time': 0.0071, 'eval_runtime': 634.9227, 'eval_samples_per_second': 1.575, 'eval_steps_per_second': 0.394}
2025-06-10 16:06:00,655 - INFO - Starting SFT training...
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 7,824 | Num Epochs = 4 | Total steps = 980
O^O/ \_/ \    Batch size per device = 32 | Gradient accumulation steps = 1
\        /    Data Parallel GPUs = 1 | Total batch size (32 x 1 x 1) = 32
 "-____-"     Trainable parameters = 161,480,704/7,777,097,216 (2.08% trained)
  0%|          | 0/980 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/sorgin1/users/jbarrutia006/viper/scripts/sft/sftrain-qwen.py", line 414, in <module>
    main()
  File "/sorgin1/users/jbarrutia006/viper/scripts/sft/sftrain-qwen.py", line 383, in main
    trainer.train()
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 359, in _fast_inner_training_loop
  File "<string>", line 68, in _unsloth_training_step
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/accelerate/accelerator.py", line 2246, in backward
    loss.backward(**kwargs)
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/autograd/function.py", line 307, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/cut_cross_entropy/cce.py", line 94, in backward
    grad_scale = 1 / lse.numel()
                 ~~^~~~~~~~~~~~~
ZeroDivisionError: division by zero
Traceback (most recent call last):
  File "/sorgin1/users/jbarrutia006/viper/scripts/sft/sftrain-qwen.py", line 414, in <module>
    main()
  File "/sorgin1/users/jbarrutia006/viper/scripts/sft/sftrain-qwen.py", line 383, in main
    trainer.train()
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/transformers/trainer.py", line 2171, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 359, in _fast_inner_training_loop
  File "<string>", line 68, in _unsloth_training_step
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/accelerate/accelerator.py", line 2246, in backward
    loss.backward(**kwargs)
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/torch/autograd/function.py", line 307, in apply
    return user_fn(self, *args)
           ^^^^^^^^^^^^^^^^^^^^
  File "/sorgin1/users/jbarrutia006/venvs/viper_tximista/lib/python3.11/site-packages/cut_cross_entropy/cce.py", line 94, in backward
    grad_scale = 1 / lse.numel()
                 ~~^~~~~~~~~~~~~
ZeroDivisionError: division by zero
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mqwen[0m at: [34mhttps://wandb.ai/jbarrutia006-upv-ehu/viperSFT/runs/2nr04e1h[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250610_153739-2nr04e1h/logs[0m
srun: error: localhost: task 0: Exited with exit code 1
